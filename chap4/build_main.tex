\chapter{Build}\label{ch:build}
This chapter outlines the steps of the \ac{DSR} process, which are
summarized under the ``Build'' phase, except `Identify Problem \& Motivation' which was already done in the
chapter~\ref{ch:introduction} `Introduction'.
The design science process is described in more detail in chapter
chapter~\ref{ch:research-methodology} `Research Methodology'.

The first activity in the DSR process was to identify the problem that needs to be addressed,
this step leads to the formulation of \ac{DP}, which are later used to evaluate
the effectiveness of the artefact.
The \ac{DP} are derived from the requirements of the overall models and are based on
the work of~\cite{siebert2022construction}.

%The Build phase involves the creation of machine learning models, which serve as
%artifacts.
%By implementing these steps, the DSR process establishes a rigourous and methodical
%approach for creating new artifacts.


\section{Objectives of a Solution}\label{sec:objectives-of-a-solution}

The objectives of a solution are essential to evaluate the effectiveness of machine learning (ML) models in the
DSR process.
To this end, several DPs are used, which are chosen based on the quality parameters proposed by Siebert et
al.~\cite{siebert2022construction}.
These quality parameters are specifically tailored for evaluating ML models and are, therefore, well-suited for this
study.

In the following sections six DPs will be desibed which will provide insights into the strengths and weaknesses of the
ML models and guide the decision-making process in the DSR process.
The \ac{DP} are correctnes, relevance, robustness, stability, resource utilization and interpretability.
By leveraging these principles, researches can optiize their models to achive the desired outcomes and improve the
quality of their research

In Chapter~\ref{ch:evaluation} `Evaluation' these six DP metrics will be combined with relevant questions and metrics to
assess the effectiveness of the ML models.
This approach will ensure a thorough evaluation of the ML models based on the quality parameters that are important
for the DSR process.

\subsection*{Design Principle 1: Correctness}\label{correctness}

The demand for high-quality products is a driving force in the manufacturing industry,
as consumers and businesses alike demand products that meet stringent standard of
precision and accuracy~\cite[p. 1]{cruz_applicationmachinelearning_2021}.
This is particularly true in the case of sheet metal forming, where even small
deviations from the desired dimensions can cause significant implications for the final
product.
Therefore, metal parts need to be produced with high precision and accuracy and
spring back is an undesired side effect which needs to be
minimized~\cite[p.1]{cruz_applicationmachinelearning_2021}.

\ac{ML} has emerged as a powerful tool for predicting the spring back in other bending
methods as shown in section~\ref{sec:state-of-research}~state of research.
A \ac{ML} model should predict the spring back with a high degree of accuracy and
correctness, as even small errors in the prediction will cause fitting problem in the
manufacturing process.

\subsubsection*{Design Principle 2: Relevance}
In addition to measure the correctness it is important to understand ``why''
the model has this performance.
A model is considered relevant when it is able to accurately predict the outcomes of
new unseen dat that were not used during training.
A relevant model should have a low error rate on both the training data and the test
data and achieve a good bias-variance trade-off~\cite[p. 16]{siebert2022construction}.

Understanding the bias-variance trade-off is crucial in developing \ac{ML} models that
generalize well to unseen data~\cite[p. 49--51]{zhou_machinelearning_2021}.
It suggest, that a model can accurately capture the underlying patterns in the data
without overfitting or underfitting~\cite[p. 49--51]{zhou_machinelearning_2021}.

\subsubsection*{Design Principle 3: Robustness}

When working with real-world data, quality issues such as outliers, missing data, and
noise are common problems.
These issues can have a negative impact on the performance of the model, making it challenging to produce accurate
predictions.
To overcome these challenges, a robust model must be able to handle data quality issues and still produce
accurate predictions~\cite[p. 16]{siebert2022construction}.

%\cite{saez_evaluatingclassifierbehavior_2016} proposed a new measure called Equalized
%Loss of Accuracy to evaluate classifications models for robustness.
%Because regressions algorithms are used in this new metrics have to be found~\cite[p.
%3]{saez_evaluatingclassifierbehavior_2016}.

\subsubsection*{Design Principle 4: Stability}

The stability of a model is an important quality parameter, as it is essential that
the model produces the same results when trained on different datasets.
This is particularly important in the case of \ac{ML} models, as the training data
is often limited and the model is trained on a small subset of the available data.
Therefore, it is important that the model is able to generalize well to unseen data and
produce repeatable results when trained on different
datasets~\cite[p. 16]{siebert2022construction}.

\subsection*{Design Principle 5: Resource utilization}
One of the primary objectives of utilizing machine learning for predicting spring back is to
reduce the number of trial-and-error cycles during the manufacturing process, thus saving
valuable resources such as material and labor
(refer to Section~\ref{sec:problem-identification-and-motivation}).

As example the technology sighted in section~\ref{sec:technology-tables} require 11 unique bends to get the data
for a for one type of metal with a certain thickness.
These metal sheets cannot be repurposed due to their specific angles resulting in wasted materials and manual labor.
Additionally, the machines used for bending the metal are often occupied around the clock, further adding to the
inefficiency of the process (personal communication, Dr. Wolfram Hochstrate).

Nevertheless, it is crucial to acknowledge that developing an \ac{ML} model also entails resource
consumption.
Training \ac{ML} models can be a labor-intensive process, as it often requires significant
computing power and the generation of large amounts of training data, depending on the algorithm
used.
Resources used up by \ac{ML} are for example memory space and also time needed to train the model and make predictions.

Therefore, it is crucial to carefully consider the resources needed to train and use the model
effectively~\cite[p. 16]{siebert2022construction}.
What resources are used and evaluated is described in more detail in~\ref{sec:resource- utilization}.

\subsubsection*{Design Principle 6: Interpretability}
Interpretability is a critical design principle that refers to the degree to which humans can comprehend and
interpret the decision-making process of a trained machine learning model~\cite[p. 13]{molnar2020interpretable}.
\cite{miller2019explanation} describe interpretability as the ``degree to which a human can understand the cause
of a decision''~\cite[p. 1]{miller2019explanation}.
Good interpretability is crucial because it fosters trust in the model and enables users to rely on it.
Additionally, interpretability can aid in debugging and enhancing the model's performance.

In this study it is the goal to comprehend the decision making process of the trained models as good as possible.
As discussed in Chapter~\ref{ch:theoretical-foundations}, the sheet metal forming process involves numerous
variables, making the process design intricate, particularly when producing components that necessitate several
processing stages multiple sets of tools~\cite[p. 1]{dib_singleensembleclassifiers_2020}.
Therefore,  and interpretable model is deemed a better model and enables the user to make informed decision based
on the models results.


\section{Dataset generation}\label{sec:dataset-generation}

The dataset was generated by conducting a series of bending experiments on metal sheets with varying thicknesses.
Cold-rolled steel conforming to the DIN EN 10130 standard was used as the material, with thicknesses ranging from 0
.5 mm to 3 mm.
This material was chosen due to its widespread use in bending processes and its wide availability.

To create the dataset, 20x100 mm sheets were utilized along with a stamping press from the company Zwick.
The metal sheets were bent in the rolling direction, as the spring back effect varied depending on the rolling
direction.

The original dataset comprised 384 \texttt{.tra} files, which is a proprietary format used by the stamping press.
Python scripts were developed to convert the output data format from the machine to more accessible CSV files.

The following section outlines the experimental setup used for the experiments performed.
Data collection occurred outside of the thesis period and initially consisted of 384 samples.
An additional \texttt{X} samples were later added to the dataset to fill any gaps present in the dataset.


% Pado: Since the data collection was technically done outside of the thesis
% period, 4.2.1
% (Multiple cycles) can be
% omitted. 4.2.2 is needed in order to understand the data set.

%\subsection{Preliminary Tests}
%A number of preliminary tests were conducted to determine the influence of
%the punch penetration
%on the spring back.
%
%\subsubsection{Multiple Cycles}
%One approach was to test if multiple spring back can be measure using only
%one sheet.
%Therefore, the machine was programmed to perform multiple cycles in one
%attempt and bend the
%metal sheet multiple
%times. The benefit of this approach would have been a faster generation of
%the dataset because
%spring backs could be
%measured in on attempt, also less material would have been used.
%
%Figure~\ref{springback_multiple} shows one of these attempts. The metal
%sheet was bent 4 times
%using $y_p$ values
%from 5 to 8. The results show, that 4 different spring backs can be
%measured, but the spring back
%does not vary like
%expected. It was observed as well, that the spring backs are different in
%every attempt, this is
%shown in
%Figure~\ref{springback_multiple_inconsistent_results}.
%Bending 4 different metal sheets each only one time returned very different
%results.
%A possible explanation could be the cold deformation of the steel, which is
%not reversible.
%Because this approach did
%not work, the machine was programmed to perform one cycle at a time.
%
%\captionsetup{width=0.45\textwidth}
%
%\begin{figure}[H]
%    \centering
%    \begin{minipage}[b]{0.5\textwidth}
%        \centering
%        \includegraphics[width=0.9\textwidth]{springback_multiple.jpg} %
%first figure itself
%        \caption{Experiment: Bending one metal sheet multiple times with
%different $y_p$ values.}
%        \label{springback_multiple}
%    \end{minipage}\hfill
%    \begin{minipage}[b]{0.5\textwidth}
%        \centering
%        \includegraphics[width=1
%.1\textwidth]{springback_multiple_inconsistent_results.png} %
%        second figure itself
%        \caption{Inconsisten results bending one metal sheet mutliple times.
%The spread of the
%        results is very large.}
%        \label{springback_multiple_inconsistent_results}
%    \end{minipage}
%    \label{fig:springback_multiple_overview}
%\end{figure}

%\subsubsection{Bending Machine}
%Before using the three point bending machine, a brake bending machine was
%used to test the
%influence of the bending
%on the spring back. The brake bending machine is a machine used to bend
%metal sheets. It is a
%very common machine in
%the industry and is used to bend metal sheets to a specific radius. The
%brake bending machine
%used is a
%\textit{Bendmaster 1000} from \textit{Bendmaster}.
%
%After a series of bends it was observed, that the spring back values where
%much higher than
%expected. The explanation
%for that behavior was, that altering the position the bending beam of that
%specific machine was
%not enough to get the
%desired angle. Thus, the machine excluded for the generation of the data and
%the three point
%bending machine was used
%instead.
%
%Despite the inaccurate data, it was later observed, that the distribution of
%the spring backs was
%very similar to the
%later experiments with the three point bending machine.

\subsection{Experimental Setup}\label{subsec:experimental-setup}
The stamping press utilized for the experimental setup is the \textit{Zwick 1454MO}, which is equipped with a load cell
and a displacement sensor.
These measure the force applied to the sheet ($F$, measured in N) and the displacement of the punch, also called
punch penetration ($y_p$), respectively.
The die is mounted on the bottom and is stationary, while the punch is mounted on the top and can be moved (see
Section~\ref{subsec:sheet-metal-manufacturing} for more details).
The die opening of the machine is adjustable from 10 mm to 100 mm.
The machine is operated via a computer and the \textit{ZwickRÃ¶ll TestXpert} software, which is used for both machine
control and data collection.

The process parameters and experimental setup are illustrated in Figure~\ref{fig:process_parameters}.
The die opening, $V$, represents the distance between the two contact points of the die, where the sheet metal is
placed.
The punch penetration, $y_p$, measured in millimeters, is the distance the punch moves into the sheet and
the thickness of the metal sheet is denoted by $t$ and ranges from 0.5 to 3 mm in this study.
The radius of the punch tip, denoted as $r_p$, remains constant throughout the experiments as it was never replaced.
The bending angle, $\alpha$, is provided for completeness but will not be used in the subsequent analysis.

\begin{figure}[h]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \centering
        \includegraphics[trim=left botm right top, width=0.6\textwidth,
            clip]{chap4/images/process_parameters}
        \caption{\textbf{Process parameters:} Sheet bending angle ($\alpha$), sheet
        thickness ($t$), punch
        penetration ($y_p$), die opening ($V$) and punch radius ($r_p$)}
        \label{fig:process_parameters}
    \end{tcolorbox}
\end{figure}

To ensure consistent results, a set of constant and variable parameters were selected.
The punch was never changed and therefore the punch radius ($r_p$) is a constant parameter.
Also the length and the width of the used metals sheet were standardized to 80 mm and 20 mm, respectively.

The hold time, which refers to the duration that the punch remains stationary
after reaching the maximum displacement ($y_p_{\max}$), was set to a minimum of 1 second.
The punch force threshold was set to 1 N, meaning that the punch was initially moved at
a higher speed until the force reached 1 N, and then moved at a slower speed of 80 mm
/min until the $y_p_{\max}$ was reached.

\begin{table}[htb]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \sisetup{group-minimum-digits = 4}
        \centering
        \label{tab:experimental-setup-constant-parameters}
        \begin{tabular}{lll}
            \toprule
            \thead{\textbf{Parameter}} & \thead{\textbf{Values}} &
            \thead{\textbf{Unit}}
            \\
            \midrule
            Punch radius & 5 & $mm$
            \\
            \hdashline
            Sheet width & 20 & $mm$
            \\
            \hdashline
            Sheet length & 100 & $mm$
            \\
            \hdashline
            Punch speed & 80 &
            $mm/min$ \\
            \hdashline
            Punch speed up (after bend) & 8 &
            $mm/min$ \\
            \hdashline
            Hold time & 1 & $s$ \\
            \hdashline
            Punch force threshold & 1 & $N$
            \\
            \bottomrule
        \end{tabular}
        \caption{Constant parameters in th eperimental setup}
    \end{tcolorbox}
\end{table}

The experiment involved varying three parameters: The die opening ($V$), the
punch penetration ($y_p$), and the thickness of the metal sheet ($t$). The die opening
was varied from 10 mm to 50 mm, while the maximum punch penetration was varied from 2
.5 mm to 20 mm.
The thickness of the metal sheet was also varied, with values ranging from 0.5 mm to 2 mm.
These parameters and their values can be seen in
Table~\ref{tab:experimental-setup-variable-parameters}.

\begin{table}[htb]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \sisetup{group-minimum-digits = 4}
        \centering
        \begin{tabular}{lll}
            \toprule
            \thead{\textbf{Parameter}} & \thead{\textbf{Values}} &
            \thead{\textbf{Unit}}
            \\
            \midrule
            %  \unit{(Kcal\per\mole)\squared}}} & \thead{RMSD l.b.} &
            %  \thead{RMSD u.b.}  \\
            \midrule
            Punch penetration  $y_p$ & 2.5, 5, 7.5, 10, 12.5, 15, 17.5, 20 &
            mm \\
            \hdashline
            Die opening        $V$ & 10, 20, 30, 40, 50
            & mm \\
            \hdashline
            Thickness          $t$ & 0,5, 1, 1.5, 2, 2.5, 3
            & mm \\
            \bottomrule
        \end{tabular}
        \caption{Varying parameters in the experimental setup}
        \label{tab:experimental-setup-variable-parameters}
    \end{tcolorbox}
\end{table}

\subsection{Measuring The Spring Back} \label{subsec:measuring_the_spring_back}
The output data included various data points that were used to calculate the spring back.
Key parameters for this calculation were the force, punch travel, and time
which are illustrated in figure~\ref{fig:springback_measured}.
The blue line is representing the for force while the gray line is representing the punch travel.

The punch movement as soon as the displacement sensor is activated with a for of 1 N, this means that the punch
touched the metal sheet.
It an be seen that before the punch touches the metal sheet there is already a force measured of about 10 N, this is
the case because the punch is moving at a speed of 80 mm/min which is relatively fast and so the displacement sensor

The wait time at of 1 second $y_p_{max}$ is a limitation of the machine and can not be
changed and therefore is always a part of the experimental setup.
At $y_p_{max}$ force decrease bee seen in figure~\ref{fig:springback_measured}, which is most
likely due to released friction (personal communications with ...).

\begin{figure}[H]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \centering
        \includegraphics[width=0.7\textwidth]{chap4/images/springback_measured}
    \end{tcolorbox}
    \caption{\textbf{Spring back measured:} A steel metal sheet was bent with a punch penetration of 5
    mm the spring back is 0 .37 mm. The blue line shows the force and the blue line shows the punch penetration.
    The distance between the green and the red point repesents the spring back distance.}
    \label{fig:springback_measured}
\end{figure}

\subsection{Dataset Exploration}\label{subsec:dataset-exploration}
The dataset was explored using the python library \textit{pandas}~\cite{mckinney-proc-scipy-2010}
and the \textit{matplotlib}~\cite{Hunter:2007} library.
The goal of this section is to give the reader an overview of the dataset and
to show the relationship between the features and the dependent variable.

\subsubsection{Features}
The output data of the bending machine consisted of 26 features, which are listed in
the appendix.
Only three features - force, distance $y_p$, and time - are relevant for
calculating the spring back, as described in the previous section~\ref{subsec:measuring_the_spring_back}.
These three features were combined with the calculated spring back to form the final
dataset, which contained a total of 396 data points generated.
An example of the dataset is presented in Table~\ref{tab:dataset_example}.

\begin{table}[h]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \sisetup{group-minimum-digits = 4}
        \centering
        \begin{tabular}{l|llll}
            \toprule
            \thead{\textbf{index}} & \thead{\textbf{Punch Penetration}} &
            \thead{\textbf{Spring Back}}
            &
            \thead{\textbf{Thickness}}
            & \thead{\textbf{Die Opening}}
            \\
            1   & 5.0  & 0.6667 & 2.0 & 50  \\
            \hdashline
            2   & 15.0 & 0.9164 & 2.0 & 50  \\
            \hdashline
            3   & 10.0 & 0.6829 & 2.0 & 50  \\
            \hdashline
            ... & ...  & ...    & ... & ... \\
            \hdashline
            396 & 5.0  & 0.6667 & 3.0 & 10  \\
            \bottomrule
        \end{tabular}
    \end{tcolorbox}
    \caption{\textbf{Dataset example:} The dataset contains 396 data points. The independent variables are the punch
    penetration, thickness and die opening. The dependent variable is the spring back.}
    \label{tab:dataset_example}
\end{table}

The figure presented in Figure~\ref{fig:v30_springbacks} depicts the spring backs that were observed in a specific
subset of the data where the die opening is 30 mm.
The horizontal axis, labeled as the x-axis, represents the punch penetration distance, while the vertical axis,
labeled as the y-axis, represents the spring back. In order to clearly illustrate all the data points, the mean of
the spring backs has been plotted as a line.

Upon examining the figure, two general trends become evident. Firstly, a lower thickness of the metal sheet leads to
a higher degree of spring back. Secondly, a higher punch penetration distance leads to a higher spring back as well.
It is also apparent that there is a non-linear relationship between the punch penetration and the spring back.
These trends are also observed for other die openings and therefore apply for the whole dataset.

%In the chosen example it chan be observed that the spring back of the metal sheets with $t = 0.5$
%are significantly higher then the other thicknesses.
%Despite the availability of data, the factors underlying the spring back behavior
%remain incompletely understood.
%Because too many factors are involved in the bending process.

\begin{figure}[h]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \centering
        \includegraphics[width=0.8\textwidth]{chap4/images/all-springbacks-consolidated}
        \caption{All measured springbacks for the die opening $V30$ are shown. }
        \label{fig:v30_springbacks}
    \end{tcolorbox}
\end{figure}

\subsubsection{Data Quality}
The dataset was carefullyj created in a controlled experimental environment to ensure precise and accurate
measurements of the samples.
As a result, the dataset exhibits a minimal number of outliers and high data quality,
which is a crucial prerequisite for developing reliable machine learning models.

Figure~\ref{fig:train_test_split} illustrates that the dataset encompasses nearly all possible combinations of the
process parameters, namely $V$, $t$, and $y_p$.
The $y_p$ values in the dataset are uniformly distributed and consistently range between 2.5 and 20 mm.
Furthermore,the dataset was continuously expanded with new data points throughout the project to enhance its size and
diversity.
The following gaps can be seen in the dataset: Metal sheet thickness with $V10$  after 1 mm could not be measured
because the die opening was too small to fit the metal sheet.
Also data for $V$ 20 and $t$ 2,5 as well as $V$ 40 and $t$ 1 are missing and might be added in the future.

Throughout the data collection and expansion process, any outliers and incorrect measurements were identified and
removed from the dataset to maintain its high quality.
However, it is worth noting that the data may not fully reflect real-world scenarios where various factors, such as
measurements errors, noise and bias can impact data quality.
In later sections, data quality issues will be intentionally introduced to evaluate the models' robustness to such
challenges.
This will provide insights into how well the machine learning models perform in more realistic scenarios and aid in
developing more effective models for practical applications.

\subsection{Data Preprocessing}\label{subsec:data-preprocessing}
To normalize the independent features $y_p$, $V$, and $t$, as well as the dependent feature $spring_back$, the
\texttt{MinMaxScaler} or, in some cases, the \texttt{StandardScaler} from the
\texttt{scikit-learn}~\cite{scikit -learn} library was employed.

The \texttt{MinMaxScaler} scales the data between 0 and 1, and the scaler was fit on the training data and
subsequently used to transform the test data. It is important to note that scaling was only performed on the training
data, as cross-validation was later employed to tune and evaluate the models. Scaling the entire dataset before the
split could result in data leakage, as the minimum and maximum values of the test data would be used to scale the
training data.

The data split is depicted in Figure~\ref{fig:train_test_split}.

\subsection{Computational Setup}\label{subsec:computational-setup}
For training the machine learning models a ThinkPad X1 Carbon 2019 with an
Intel Core i7-10610U CPU @ 1.80GHz and 16 GB RAM was used.
The operating system used is Ubuntu 20.04.2 LTS. The code for the model is
written in Python 3.8.5 using the IDE PyCharm.
The code can be found on GitHub and is linked in the appendix.
The libraries used are mentioned in Table~\ref{table:libraries}.

\captionsetup{width=1\textwidth}

\begin{table}[h]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \sisetup{group-minimum-digits = 4}
        \centering
        \begin{tabular}{llll}
            \toprule
            \thead{\textbf{Library}} & \thead{\textbf{Version}} & \thead{\textbf{Author}} \thead{\textbf{Use}}
            &
            \\
            \toprule
            Sci-Kit Learn & 1.2.2 & ~\cite{scikit-learn} &
            Machine Learning in python \\
            \hdashline
            numpy~ & 1.23.2 & ~\cite{harris2020array} &
            Mathematical operations in python \\
            \hdashline
            pandas & 1.5.1 & ~\cite{mckinney-proc-scipy-2010} & Data
            manipulation \\
            \hdashline
            matplotlib & 3.6.2 & ~\cite{Hunter:2007} &
            Plotting data \\
            \hdashline
            scienceplots & 2.0.1 & ~\cite{SciencePlots} &
            Style of matplotlib \\
            \bottomrule
        \end{tabular}
    \end{tcolorbox}
    \caption{Libraries used for the machine learning models.}
    \label{table:libraries}
\end{table}

Upon examining the correlation matrix depicted in Figure~\ref{fig:correlation_matrix},
it is evident that the distance and spring back features exhibit a stronger correlation
than the other features.
This correlation is to be expected since punch penetration $y_p$ is the primary factor
that influences the amount of spring back.
It is noteworthy that the other features are not correlated with each other, indicating
the absence of multicollinearity, which is a desirable trait for machine learning models.

\begin{figure}[H]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \centering
        \includegraphics[width=0.7\textwidth]{chap4/images/correlation_matrix}
        \caption{Correlation matrix}
        \label{fig:correlation_matrix}
    \end{tcolorbox}
\end{figure}


\section{Model Selection}\label{sec:model-selection}
This thesis focuses on the utilization of supervised machine learning models to predict
spring back.
Therefore, a selection of the most common machine learning models where used, based on
the systematic literature review of~\cite{dridi2021supervised}.
In the paper, the author defines eight widely used supervised machine learning models: Logistic
Regression, Support Vector Machines, Decision Trees, Random Forests, AdaBoost, Naive Bayes,
and K -Nearest Neighbors~\cite[p. 8]{dridi2021supervised}.
However, Naive Bayes and K-Nearest Neighbors are not used in this thesis since they are typically
utilized for classification problems.
Furthermore, in order to compare the performance of the other models, a Linear Regression model
and a \ac{MLP} were also employed.
The models can be classified into five categories: Logic-based learning, instance-based learning, Deep learning, and
Support Vector Machines~\cite[p. 8]{dridi2021supervised}.

\subsection{The Baseline Model}\label{subsec:regression-models}
In this study, a linear regression model is employed as the baseline model.
Linear regression a widely used method that is used to predict a continuous output (dependend variables) based on a set
of input features (independend variables).
The goal of a \ac{LR} model is to find the best-fitting straight to explain the relationship between the dependent
and independent variables~\cite[p. 45]{muller_introductionmachinelearning_2016}.
This so called regression line can be express as shown in Equation~\ref{eq:linear-regression}.

\begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
    \begin{equation}
        \hat{y} = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b
        \label{eq:linear-regression}
    \end{equation}
\end{tcolorbox}

In this scenario, $x[0]$ to $x[p]$ represents the characteristics of one instance of data, with $p$ being the number of
features.
The model's parameters, $w$ and $w$, are acquired through learning, and $\hat{y}$ signifies the forecast made by the
model.
If the dataset only has one feature, the equation would appear as:

\begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
    \begin{equation}
        \hat{y} = w[0] * x[0] + b
        \label{eq:linear-regression-one-feature}
    \end{equation}
\end{tcolorbox}

The linear regression model is trained by minimizing the sum of squared errors between the predicted and actual
values.

The mean squared error is calculated by summing the squared differences between predicted and true
values~\cite[p. 47--68]{muller_introductionmachinelearning_2016}.
This model assumes that the relationship between the independent and dependent variables is linear.
However, the dataset exploration in Section~\ref{subsec:dataset-exploration} revealed
a nonlinear relationship, suggesting that the linear regression model may not provide accurate predictions.
Despite this limitation, the model can be used as a baseline for comparison against other models.
Another reason to use linear regression is its interpretability and simplicity.
The linear relationship between inputs and targets makes model interpretation
straightforward~\cite[p. 37]{molnar2020interpretable}.

To train the linear regression model, the sum of squared errors between the predicted values and
the actual values is minimized.
The mean squared error is determined by summing the squared differences between the predicted
values and the true values~\cite[p. 47--68]{muller_introductionmachinelearning_2016}.

The model is based on the assumption that the relationship between the independent
variables and the dependent variable is linear.
Since in the dataset exploration (section~\ref{subsec:dataset-exploration}) it was found that the relationship
between the independent variables and the dependent variable is not linear, the linear regression model
is not expected to produce accurate results.
However, it still is useful to be used as a baseline model in order to compare the other models that will be trained.

Another reason why an \ac{LR} model is used in this study is its interpretability and simplicity.
The linearity of the relationship between the inputs and the target makes the
interpretation of the model straightforward~\cite[p. 37]{molnar2020interpretable}.

\subsection{Support Vector Regression (SVR)}\label{subsec:support-vector-regression-(svr)}
Support Vector Machines (SVMs) are a popular approach for solving classification problems.
To understand how the SVM algorithm works, it can be helpful to approach the problem from a geometric perspective.
For example, a two-dimensional space as shown in Figure~\ref{fig:svm-example} can be considered, where each data point
has a class assigned, in this case, either red or blue.
The SVM algorithm seeks to identify a line (hyperplane) that separates the data points into different classes in the
best possible way which means that there is the largest possible margin between the line and the
nearest points of each class~\cite{muller_introductionmachinelearning_2016}.
The margin is defined as the distance between the hyperplane and the nearest point of each class.
The math behind that is quite involved and is not covered in this thesis, interested readers can read more about it
in ~\cite{hastie2009elements}.

The points that lie closest to the hyperplane are called support vectors, these define the margin which is tried to be
maximized by the algorithm and therefore play a crucial role in determining the optimal position of the hyperplane.
The hyperplance servers as a decision boundary that spearates the data points into different
classes, in the example the points on the one side of the hyperplane are classified with the blue class while the
points on
the other side are classified with the red class ~\cite[p. 42]{awad_efficientlearningmachines_2015}.

Since each feature in the dataset is formulated as one dimension in the geometrical space, visualizing the hyperplane
is usually not as straightforward as in the example depicted in Figure~\ref{fig:svm-example}. In the case of the
spring back dataset, for instance, the hyperplane would be a three-dimensional plane, but it is not uncommon to have
spaces with even higher dimensions.
The algorithm aims to identify a hyperplane in an N-dimensional space, where N denotes the number of features, to
effectively separate and classify data points~\cite[]{awad_efficientlearningmachines_2015}.

\begin{figure}[H]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \centering
        \includegraphics[width=0.7\textwidth]{chap4/images/svm_example}
        \caption{Simple \ac{SVM} examle~\cite[p. 94]{muller_introductionmachinelearning_2016}}
        \label{fig:svm-example}
    \end{tcolorbox}
\end{figure}

However, for regression problems such as predicting spring back, the \ac{SVM} algorithm
needs to be adapted to provide continuous values instead of a finite set of values.
To address this, the Support Vector Regression (SVR) algorithm was developed, which draws inspiration from the
\ac{SVM} algorithm and leverages similar principles.
It fits a model to data by only considering residuals smaller in absolute value
than a designated constant known as $\epsilon$-sensitivity.
By doing so, \ac{SVR} can accurately model and predict continuous values, making it a
suitable approach for regression problems.

To enable the \ac{SVR} algorithm to generate continuous predictions, it creates a ``tube''
while the points outside the tube are penalized based on their distance from the
predicted function.
This approach is similar to how \ac{SVM}s penalize points in classification
problems~\cite[p. 369]{montesinoslopez_supportvectormachines_2022}.

% Kernel trick
To transform the data into a higher dimensional space a method called kernel trick is used.
Two methods are usually used for \ac{SVM}s, the polynomial kernel and the radial basis
function, also known as gaussian kernel~\cite[p. 97--98]{muller_introductionmachinelearning_2016}.
Similar to the \ac{SVM} algorithm, the \ac{SVR} algorithm finds a well-fitting hyperplane to a
kernel-induced feature space to achieve good generalization performance using the original
features.
This is detailed in~\cite[p. 369]{montesinoslopez_supportvectormachines_2022}.

The hyperparameters used for the algorithm in this work are listed in
table~\ref{tab:hyperparameters_svr}.

\begin{table}[H]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \sisetup{group-minimum-digits = 4}
        \centering
        \label{tab:hyperparameters_svr}
        \begin{tabular}{ll}
            \toprule
            \thead{\textbf{Hyperparameter}} & \thead{\textbf{Value}} &
            \thead{\textbf{Description}}
            \\
            %  \unit{(Kcal\per\mole)\squared}}} & \thead{RMSD l.b.} &
            %  \thead{RMSD u.b.}  \\
            \toprule
            kernel & rbf
            \\
            \hdashline
            degree & 1
            \\
            \hdashline
            gamma & 0.1
            \\
            \hdashline
            C & 4000 \\
            \hdashline
            epsilon & 0.001 \\
            \bottomrule
        \end{tabular}
    \end{tcolorbox}
    \caption{Hyperparameters of the \ac{SVR} model}
\end{table}

\subsection{Logic-based learning}\label{subsec:logic-based-learning}

Logic-based algorithms solve problems by applying logical functions sequentially or
incrementally.
An example of such an algorithm is a decision tree, which is commonly used as a classification
and regression model~\cite[p. 10]{dridi2021supervised}.

\subsubsection{Decision Trees}
% -> Problem von decision tree
Decision Trees (\ac{DT}s) are a widely used type of model for both classification and
regression problems.
These models construct a hierarchy of rules to predict outcomes based on the
data~\cite[p. 70]{muller_introductionmachinelearning_2016}~\cite[p. 253]{shaik_briefsurveyrandom_2019}.
The algorithm splits the data into multiple segments based on specific feature
valuescreates various subsets of the dataset, with each sample belonging to one subset.
The final subsets are called terminal or leaf nodes, while the intermediate subsets are
known as internal or split nodes.
To predict the outcome in a specific leaf node, the mean outcome of the training data
in that leaf node is used~\cite[p. 70--72]{muller_introductionmachinelearning_2016}.

Figure~\ref{fig:dt-example} shows a greatly reduced real-life example of a decision tree taken out of a random forest
that was trained for this study. Random forests will be explained in the next section.
At the core of the decision tree lies the root node, which serves as the starting point for the analysis.
In this particular dataset, the root node contains all 403 data points, representing the complete set of observations
in the sample.
From the root node, the decision tree branches out into two or more decision nodes, each representing a split in the
data based on a decision rule.
These decision rules are listed on the first line of each node, and provide a criterion for dividing the data into
subsets based on specific feature thresholds.
For example, in the line \texttt{x[1] <= 0.75}, the decision rule is based on the second independent variable,
$x[1]$, and the threshold value of 0 .75.

In this dataset, the independent variables are denoted as $x[i]$, where $x[0]$ represents the punch penetration,
$x[1]$ represents the metal sheet thickness, and $x[2]$ represents the die opening. These variables are used to make
decisions at each node, and provide the basis for predicting the value of the dependent variable.

One important metric for evaluating the performance of the decision tree is the squared error, or \ac{MSE},
associated with the predictions made by each decision node.
This metric indicates how well the decision node separates the data into different subsets based on the selected
feature threshold.
A lower value indicates a better separation, and suggests that the decision rule is more effective in predicting the
value of the dependent variable.
Performance metrics like the \ac{MSE} are further explained in section~\ref{correctness}.

The line labeled `samples' in each node denotes the number of samples that reach that particular decision node.
This metric provides insight into the distribution of data within the decision tree, and can be used to identify
areas of high or low density within the sample.

Finally, the `value' line in each node represents the predicted spring back for the samples that reach that
particular decision node.
This value provides an estimate of the dependent variable, and can be used to make predictions based on the input data.

\begin{figure}[h]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \centering
        \includegraphics[width=0.7\textwidth]{chap4/images/decision_tree_example}
    \end{tcolorbox}
    \caption{Decision tree example made with \texttt{sklearn.tree.plot\_tree}~\cite{scikit-learn}.
    The green noded represents the root, the blue nodes are the decision nodes, and the red nodes are the leaf nodes.}
    \label{fig:dt-example}
\end{figure}

\subsubsection{Random Forest}\label{subsubsec:random-forests}
The primary issue with Decision Trees (\ac{DT}) is their propensity to overfit, leading to poor generalization
performance and rendering them unsuitable for most use cases.
To tackle this problem, ensemble methods are often employed instead of a single
DT~\cite[p. 78]{muller_introductionmachinelearning_2016}~\cite[p. 251]{liu_newmachinelearning_2012}.
In ensemble methods, multiple models such as decision trees are combined to form a single model. To make new
predictions, the predictions of each individual model are aggregated, for example by averaging the
predictions~\cite[p. 222]{boehmke2019hands}.

The \ac{RF} algorithm build trees independent in parallel nad the final prediction is made by averaging the predictions
of all trees.

One prominent ensemble learning algorithm is the Random Forest
(\ac{RF}) algorithm~\cite[]{breiman_randomforests_2001}, which combines multiple decision trees (weak learners) to
produce a more precise and stable prediction
(strong learner)~\cite[p. 24]{awad_efficientlearningmachines_2015}~\cite[pp. 340]{gareth2013introduction}.
This approach, known as the `divide and conquer' method,involves dividing the data into smaller subsets and
constructing a randomized tree predictor for each subset~\cite[p. 251]{liu_newmachinelearning_2012}.

The risk of overfitting is mitigated through subset and feature randomization, also known as bagging.
Bagging involves randomly selecting a subset of features or variables at each decision node in the decision
tree~\cite[p. 341]{gareth2013introduction}.
This process generates multiple, slightly varying copies of the dataset.
Overfitting occurs when the algorithm learns individual data points instead of
the general pattern.
Each root node utilizes a unique subset of the data, and each leaf is split using a random set of features.
This approach ensures that no single tree is exposed to all the data, enabling the model to concentrate on general
patterns rather than being susceptible to noise~\cite[p. 251]{liu_newmachinelearning_2012}.
Therefore bagging aids in diminishing the impact of individual data points or outliers that may have a
significant influence on a single decision tree.

The algorithm is visualized in Figure~\ref{fig:rf-example}.
Instead of a single decision tree, $N$ decision trees are built in the format described in Figure~\ref{fig:dt -example}.
The bagging was not visualized but every single decision tree is trained on a varation of the dataset for the before
described reasons.
The red arrows indicate the decisions made by each decision tree for one specific sample.
As a result, each decision tree independently arrives at one prediction which in this case would be a spring back value.
The final prediction is made by averaging the predictions of all trees.

\begin{figure}[h]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \centering
        \includegraphics[width=0.6\textwidth]{chap4/images/random_forest_example}
    \end{tcolorbox}
    \caption{Visualization of the Random Forest algorithm of~\cite[p.1]{breiman_randomforests_2001} (Own
    representation).
    Green decision nodes are the root nodes, blue decision nodes are the decision nodes, and red nodes are the leaf
    nodes.
    The red arrows indicate the decisions made by each decision tree for one specific sample.}
    \label{fig:rf-example}
\end{figure}

The Random Forest mechanism is versatile enough to address both classification and regression problems, making it one
of the most successful
\ac{ML} methods~\cite[p. 3--4]{biau_randomforestguided_2016}~\cite[p. 25]{breiman_randomforests_2001}.

% -> Vorteile / Nachteile Random Forest
This mechanism is flexible enough to handle classifications and regression problems,
this is one of the reasons that random forests count to the most successful \ac{ML}
methods~\cite[p. 3--4]{biau_randomforestguided_2016}~\cite[p. 25]{breiman_randomforests_2001}.

\subsubsection{Gradient Boosting Regression Tree}

A Gradient Boosting Regression Trees (\ac{GBR}) are an ensemble learning algorithm that combines multiple decision trees
to yield more accurate and stable predictions.
The main difference to the random forest algorithm is that the trees are trained sequentially rather than in parallel,
with each tree correcting the errors of the previous tree~\cite[p. 88--89]{muller_introductionmachinelearning_2016}.
This is done with training each new tree is residual errors of the previous tree, which es the
difference between the predicted value and can also be called the loss.
One example for a metric that can be used as loss is the mean squared error (MSE) (more details in section~\ref{
    correctness}).
To train a tree on the residual errors, the process is similar to training a decision tree on any other regression
task.
Instead of using the original target variable (spring back) as the label for the training data instead the
residual errors are used.
This process sequentially boost the performance of the mode with new tree added to the
ensemble, hence the name Gradient ~\textit{Boosting}~\cite[p. 222]{boehmke2019hands}

While Random Forests (discussed in the previous section) create an ensemble of independent trees with a deep
structure, GBMs form an ensemble of shallow trees built sequentially.
Each tree in the ensemble learns from and improves upon the previous one~\cite[p. 221]{boehmke2019hands}.
The shallow trees are achieved through a process called pre-pruning, which means that the growth of a tree is halted
at a specific depth.
This provides the advantage of a smaller model that consumes less memory and facilitates faster prediction.
Generally, generating more trees enhances the overall performance of the
model~\cite[p. 88--89]{muller_introductionmachinelearning_2016}.

Furthermore, the algorithm performs well without scaling the dataset and can handle a combination of binary and
continuous features~\cite[p. 88--89]{muller_introductionmachinelearning_2016}. However, like other tree-based models
, it does not perform well on high-dimensional data.

In Figure~\ref{fig:gbr-example}, we illustrate the Gradient Boosted Regression Trees (GBRT) algorithm using a sample
dataset.
The first tree is trained on the original dataset with the target variable being the spring back. After the
construction of the initial decision tree, each sample in the dataset will have a predicted value which is likely
different from the actual spring back.
For each sample, the residual, which is the difference between the predicted and actual spring back values, is
computed.
The second tree in the sequence is then trained using these residuals as the target variable instead of the
actual spring back values.
The GBRT algorithm continues to build trees sequentially, with each tree focusing on the residuals of the preceding
tree as its target variable.
This process continues until a predefined stopping criterion is satisfied. Such criteria
may include a maximum number of trees or a threshold for minimal improvement in the final
outcome~\cite[p. 227]{boehmke2019hands}.

\textit{Add that the resituals are somehow added to the previous tree}

\begin{figure}[h]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \centering
        \includegraphics[width=1\textwidth]{chap4/images/gradient_boosting_example}
    \end{tcolorbox}
    \caption{Visualization of the \ac{GBR} algorithm adapted from~\cite[p. 222]{boehmke2019hands}}
    \label{fig:gbr-example}
\end{figure}

\subsubsection{Extra Trees}\label{subsubsec:extra-trees}
In the construction of a random forest, decision trees are developed using a randomly selected subset of features for
each node (see Section~\ref{subsubsec:random-forests}). To further enhance the randomness of the trees, random
thresholds can be employed for decision rules in addition to the random selection of
features~\cite[p. 351]{geron2022hands}.

An ensemble consisting of such highly arbitrary trees is referred to as an extra-trees (or extremely randomized trees
) ensemble.
This approach generates more diverse and less correlated trees, leading to reduced variance and increased
bias~\cite[p. 351]{geron2022hands}.
Moreover, employing \ac{ET} classifiers accelerates the training process in
comparison to conventional random forests, as determining the optimal threshold for each feature at each node is one
of the most time-consuming aspects of tree development~\cite[p. 351]{geron2022hands}.

\subsection{Neural Networks}\label{subsec:neural-networks}
Another method for supervised learning involves the utilization of Neural Networks to
perform classification and regression tasks.

While this study does not delve deep into the use of Artificial Neural Networks for predicting spring
back as this would be beyond the scope of this work.
Instead it does employ a simple multi-layer perceptron as a baseline model for comparison with the other
methods with the goal to show if the use of neural networks can be the topic of future research.

To achieve that a Multi-layer Perceptron (MLP) is trained on the dataset and the results are compared to the other
methods.
The MLP, which is the most widely used type of artificial neural network, has to be found
producing generalizable models in various fields~\cite[p.451]{taud2018multilayer}.
MLPs can be seen as an extension of linear models that engage in several rounds of processing to
reach a conclusion~\cite[p. 104]{muller_introductionmachinelearning_2016}.

The perceptron is inspired by the biological neuron the brain and is the basic processing unit of a neural
network.
Each perceptron has one or more inputs from the previous perceptions or the environment and computes a weight sum of
these inputs.
The weight determines if a perceptron triggers a certain activation function and `fires' or
not~\cite[pp. 271--273]{alpaydin2020introduction}.
The activation function is often a simple sigmoid function which determines a threshold for the perceptron to activate.

The MLP is a feedforward neural network which means that is organized into layers, with information
moving in one direction from the input layer trough the hidden layers to the output
layer~\cite{bishop1995neural}.
Each connection between neurons has a weight and perceptron within the same layer
share the same activations function which is typically a sigmoid function in hidden
layers.
The layers are the input layer, one or more hidden layers and an output
laye.
The input layer is the first layer of the network and is responsible for receiving the input data and has the same
number of nodes as the number of features in the dataset.
In this case the input layer of the neural network would consist of three perceptrons, each corresponding to one of
the independent features namely thickness, die opening and punch penetration.
The input layer does not perform any computation and only passes the input data to the next layer, which is
Figure~\ref{fig:mlp-example} is a hidden layer.

In a fully connected neural network like the MLP each neuron in the input layer is connected to each neuron in the
first hidden layer.
While doing this each neuron multiplies the input values with the weight value associated with the connections
denoted as $w_i$
The output layer of the neural network does produce the final output of the network.
The output is calculated based on the weighted output produces by the last hidden layer.

Setting the weights to a specific values is the task of the training process and is the actual `learning' part of the
MLP.
During the training the algorithm adjusts the weights of the connections between the neurons in order to minimize
the error between the predicted and actual output.

A backpropagation algorithm is typically used to train \ac{MLP} models, by
propagating errors backwards from the output layer to the input layer in order to adjust
the weights~\cite[p. 454]{taud2018multilayer}.


This is done in multiple steps (\cite{nielsen_neuralnetworksdeep_2015}):

\begin{enumerate}
    \item The input data is passed through the network, layer by layer, until it reaches the output layer and a
    result is calculated. This process is known as the forward pass.
    \item The error is then calculated by comparing the predicted output with the actual output. For example, if the
    predicted spring back is 0.5 and the actual spring back is 0.6, the error would be the difference between
    these two values.
    \item Using backpropagation, the error is propagated backwards through the network, from the output layer
    to the input layer, to calculate the gradient of the error with respect to the weights and biases of the
    network. The weights are then updated in the direction of the negative gradient, using an optimization
    algorithm such as gradient descent, to reduce the error.
    \item This process is repeated iteratively for all
    samples in the training set, updating the weights and biases after each batch, until the error function
    reaches a minimum value or a stopping criterion is met.
\end{enumerate}

In summary, backpropagation is a process that uses the error between the predicted and actual output to update the
weights and biases of a neural network. By iteratively updating the weights and biases, the network can learn to make
more accurate predictions~\cite{nielsen_neuralnetworksdeep_2015}.

Usually a weight $x_0$ is included in the perceptron model as an intercept value to increase its generality.
It is typically represented as the weight coming from an additional bias unti that always has a value of +1.

\begin{figure}[h]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \centering
        \includegraphics[width=1\textwidth]{chap4/images/mlp_example}
    \end{tcolorbox}
    \caption{Visual representation of a Multi-layer Perceptron (own representation).}
    \label{fig:mlp-example}
\end{figure}


\section{Model Training}\label{sec:model-training}
Supervised learning involves training machine learning algorithms a dataset which contains the inputs features as well
as target feature in this study spring back.
In order to evaluate how well the model generalizes to new, unseen data, it is necessary to
split the dataset into training and testing sets.
This involves taking some samples from the original dataset and using them exclusively to test the trained models.

\subsection{Training-Test Split}\label{subsec:training-test-split}
Figure~\ref{fig:train_test_split} displays the dataset partitioned into training and
testing sets for the models used in this study.
Specifically, the samples with a die
opening of 30 were reserved for testing, while the remaining data was used for training .
While a random train-test split can often yield better model performance, this
approach may not evaluate the model's ability to predict new data with a different die
opening.

The decision to use a die opening of 30 was based on its central location within the
selected dataset, allowing the models to predict this value accurately.
Furthermore, removing data from the middle of the parameter space during training is
expected to improve model performance and promote generalization to new data.

All models were trained on the same dataset.
The hyper-parameters where set using Grid-Search Cross-validation.
The full hyper-parameters for each model are listed in Table~\ref{sec:hyper-parameters}.

\begin{figure}[ht]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \centering
        \includegraphics[width=0.8\textwidth]{chap4/images/test_train_split}
    \end{tcolorbox}
    \caption{The training and test dataset. (Data especially for V40 is
    still missing)}
    \label{fig:train_test_split}
\end{figure}

In addition to the data split shown in Figure~\ref{fig:train_test_split}, a random 80
/20 split was used to train and evaluate the models.
The second split was only used as a backup method to see if the results differs a lot from the other split.


\section{Result of the Build-Phase}\label{sec:results-build-phase}

The design and development of the machine learning models have been completed, marking the end of the `Build' phase
in this study. All models were implemented using the libraries listed in Table~\ref{table:libraries}, and the source
code is publicly available on GitHub at:

\url{https://github.com/Philipp0205/Master-Thesis-Code}

Each model has its own folder under the sub-directory \texttt{learner}, containing the model and any additional files
associated with the learner. To run the model, simply execute the `main' function.

The dataset used in Section~\ref{sec:dataset-generation} `Dataset Generation' and subsequent sections can be found in
the sub-directory \textit{data/dataset}. The dataset is structured into different folders for each die opening used
to generate the data. Additional files for the dataset, such as a visualization of how the spring back was calculated
for every single bend, can also be found in the folder.