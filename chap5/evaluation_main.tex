\chapter{Evaluation}\label{ch:evaluation}
% take a step back and put your results from 4 into context.
This chapter represents the Evaluation phase encompassing the evaluation, demonstration, and
communication activities outlined in the description of the \ac{DSR} process outlined in
Chapter~\ref{ch:research-methodology} `Research Methodology'.
The \ac{DSR} process is divided into the `build' and `evaluate' phases and their respective
activities(see Figure~\ref{fig:dsr_process}).

The evaluation activity offers a comprehensive analysis of the machine learning (ML) models implemented in the
preceding chapter.
This phase involves assessing the effectiveness of the developed models in addressing the defined
problems.
During the demonstration phase, various scenarios are chosen to illustrate the potential real-world
applications of the models and their performance in those contexts.
In the communication phase, the results are presented in a manner easily understood by a non-technical audience.
The objective of this step is to effectively communicate the outcomes and implications of the research to individuals
who may lack a technical background in ML.

Table~\ref{tab:evaluation_criteria} provides an overview of the quality metrics employed for the evaluation,
structured based on the \ac{GQM} approach discussed in subsection~\ref{subsec:goal-question-metric-approach}.
The quality metrics align with the quality model from~\cite{siebert2022construction}, which in turn is grounded in
the ISO/IEC 9126 standard for software quality evaluation.
This standard has been adapted by\cite{siebert2022construction} to address the specific requirements of machine
learning models, as detailed in Section~\ref{sec:evaluation-of-machine-learning-models}.

Since~\cite{siebert2022construction} primarily focuses on evaluating classification models, the quality metrics have
been tailored to suit the needs of the regression models implemented in this thesis.
Each individual quality metric section provides further elaboration.
Table~\ref{tab:evaluation_criteria} also displays the quality metrics used for evaluating the \ac{ML} models.
The \ac{GQM} approach structures the evaluation of the \ac{ML} models, with each goal assigned a question or one or
more metrics.
The following subsections adhere to this structure and describe the quality metrics in greater detail.


\captionsetup{margin={5pt,5pt}}
\begin{table}[h]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \centering
        {\renewcommand{\arraystretch}{1}
            \begin{tabular}{p{2cm}p{8cm}p{3cm}}
                \toprule
                \thead{\textbf{Goal}} & \thead{\textbf{Question}}
                & \thead{\textbf{Metric}} \\
                \toprule
                \textbf{Correctness} & Ability of the model to perform the
                current task measured on the development dataset and the runtime dataset~\cite[p
                . 16]{siebert2022construction}
                &
                MAE, \newline MSE, \newline RMSE
                \\
                \hdashline
                \textbf{Relevance} & Does the model achieve a good bias-variance
                tradeoff? Which means neither overfitting or unterfitting the
                data.~\cite[p. 16]{siebert2022construction}
                & Variance of CV, \newline $R^2$
                \\
                \hdashline
                \textbf{Robustness} & Ability of the model to outliers, noise
                and other data quality issues~\cite[p. 16]{siebert2022construction}
                & Loss of Accuracy, \newline Average Loss
                \\
                \hdashline
                \textbf{Stability} & Does the artifact generate repeatable
                results when trained on different data?~\cite[p. 16]{siebert2022construction}
                & LOOCV stability
                \\
                \hdashline
                \textbf{Interpret- ability} & How well can the model be
                explained?~\cite[p. 16]{siebert2022construction}
                & Linearity, \newline monotonicity, \newline  interaction
                \\
                \hdashline
                \textbf{Resource utilization} & How much resources are
                required to train and run
                the model?~\cite[p. 16]{siebert2022construction}
                & Training time, \newline runtime, \newline storage space
                \\
                \bottomrule
            \end{tabular}
        } % renew command
    \end{tcolorbox}
    \caption{Overview of the goals, questions and metrics for the
    evaluation of artifacts
    following the \ac{GQM} approach.}
    \label{tab:evaluation_criteria}
\end{table}


\section{DP1: Correctness}\label{sec:dp1:-correctness}
% Ability of the model to perform the current task measured on the
% development dataset and the
% runtime dataset

The model must be able to perform well on the selected task.
\cite{siebert2022construction} used the classification metrics precision, recall and F-
score to evaluate the correctness models.
As the regression models implemented in this thesis are not able to predict
the class of a sample, these metrics are not applicable.

To assess how accurately a statistical learning technique predicts data, a means of measuring how closely
its predictions correspond to the observed data is needed.
In other words, we need to measure how well the predicted response value for a specific observation matches the
actual response value~\cite[p. 29]{hastie2009elements}.

Commonly used metrics for regression models are the mean absolute error \ac{MAE},
\ac{MSE} and \ac{RMSE}.
Their formal description is given in~\ref{eq:mae},~\ref{eq:mse} and~\ref{eq:rmse} where
$e_i$ being the prediction error which is the difference between the predicted value
by the model and the actual value.
$y_i$ is the actual value and $n$ is the number of samples in the testing data set.
For the evaluation the \texttt{sci-kit learn}~(\cite{scikit-learn}) implementation of
these metrics was used.

The MAE is determined by computing the average of the absolute differences between the predicted and actual values.
To ensure that the errors are always positive, the absolute value of the error $e_i$ is taken, as shown in
Formula~\ref{eq:mae}~\cite[p. 1248]{chai2014root}.
The MAE is a measure of the average magnitude of errors in a set of predictions, without considering their direction.
As it returns the same units as the data, it is easy to interpret.
A lower MAE indicates better model performance, as stated by Chai and Draxler (2014)~\cite[pp. 1248]{chai2014root}.

\begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
    \begin{equation}
        MAE = \frac{1}{n} \sum_{ichat=1}^{n} |e_i|
        \label{eq:mae}
    \end{equation}
\end{tcolorbox}

The MSE is computed like shown in Equation~\ref{eq:rmse}~\cite[p. 1248]{chai2014root} (inside the root) by determining
the average of the squared differences between the predicted and actual values.
As it is based on squared differences, it is more sensitive to outliers than the MAE.
Moreover, the MSE is expressed in squared units of the data and not in the same units as the data.
If the predicted responses closely align with the actual responses, the MSE will be low, but if there is a significant
difference between the predicted and actual responses for some observations, then the MSE will be
high~\cite[p. 30]{hastie2009elements}.

The RMSE is the square root of the MSE and measures the average distance between the predicted and actual values.
It is the most widely used metric for evaluating the performance of regression models and is expressed in the same units
as the response variable.
Like the MSE, the RMSE is more sensitive to outliers than the MAE.
The MAE, on the other hand, is calculated by computing the average of the absolute difference between the predicted
and actual values.

\begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
    \begin{equation}
        \label{eq:rmse}
        MSE = \sqrt{\frac{1}{n} \sum_{i=1}^{n} e_i^2}
    \end{equation}
\end{tcolorbox}

There is an ongoing debate regarding the best metric to use for evaluating regression models.
For instance,~\cite{willmott2005advantages} contend that the RMSE is not an appropriate measure for determining the
average performance of a model and suggest using the MAE instead, while~\cite{chai2014root} argue in favor of the
RMSE.
The MAE is commonly used when a dataset contains outliers, because the RMSE is more sensitive to outliers.
However, as the dataset reviewed in Section~\ref{subsec:dataset-exploration} does not contain many outliers, the
evaluation will focus on the RMSE.
Both measures will be included in the analysis, to give the reader a better understanding of the performance of
the models.
The MSE was not included in the analysis, as it is not expressed in the same unit as the data and is therefore
more difficult to interpret and might be misleading.

\subsection{Results}\label{subsec:results}
Table~\re{tab:results-correctness} presents the results of the evaluation of the machine learning models for
correctness, sorted by the RMSE in ascending order.
As the RMSE and MSE have the same units as the dataset, in this case, the spring back in millimeters, it can be
concluded that the best models have an RMSE below 0.2 millimeters, indicating that they can predict the spring back
with acceptable accuracy for many use cases.

The Linear Regression and Decision Tree models have poor predictive performance compared to the other models.
For \ac{LR} This is expected since the relationship between the independent and dependent variables is
not linear. In contrast, Decision Trees can perform poorly when overfitting the training data, and they are prone to
instability. These problems are mitigated by using ensemble methods like Random Forest and Gradient Boosted Tree.

As the simple Decision Tree and Linear Regression models are not able to predict the spring back accurately enough,
they are not considered for the next steps of the evaluation. On the other hand, the Ada Boost, Random Forest, and
Gradient Boosted Tree models demonstrate moderate predictive performance compared to all models.

The MLP, Extra Trees, and SVM models have relatively low MAE and RMSE values, indicating good predictive performance .

It is worth noting that the SVM model has a slightly better MAE while still maintaining a comparable RMSE compared
to the other models.
This could be because the errors made by the SVM are evenly distributed across instances or
because the errors made by the other models are large for a few instances but smaller for the rest.

% Table wit hall used machine learning models and their metrics
\begin{table}[h]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
% \sisetup{group-minimum-digits = 4}
        \centering
        \begin{tabular}{lll}
            \toprule
            \thead{\textbf{Model Name}} & \thead{\textbf{MAE}}
            & \thead{\textbf{RMSE}} \\
            \toprule
            \textbf{Decision Tree}            & 0.255 & 0.348 \\
            \hdashline
            \textbf{Linear Regression}        & 0.220 & 0.282 \\
            \hdashline
            \textbf{Ada Boost}                & 0.202 & 0.261 \\
            \hdashline
            \textbf{Random Forest}            & 0.159 & 0.221 \\
            \hdashline
            \textbf{Gradient Boosted Trees}   & 0.168 & 0.221 \\
            \hdashline
            \textbf{Multi-Layered-Perceptron} & 0.136 & 0.190 \\
            \hdashline
            \textbf{Extra Trees }             & 0.135 & 0.172 \\
            \hdashline
            \textbf{Support Vector Machine}   & 0.106 & 0.146 \\
            \bottomrule
        \end{tabular}
    \end{tcolorbox}
    \caption{Results for the Desin Principle 1 Correctness. The results are sorted by RMSE in ascending order.}
    \label{tab:results-correctness}
\end{table}

The overall performance of the machine learning modes are good and the best performing models are the Extra Trees and
Support Vector Machine models.


\section{DP2: Relevance}\label{sec:relevance}
% Does the model achieve a good bias-variance tradeoff? Which means neither
% overfitting or
% unterfitting the data.

% Bias variane trade-off
A model is considered relevant when it achieves a balance between bias and
variance, avoiding both overfitting and underfitting of the training data.
The relevance of the model can be quantified through the \textit{variance of cross-validation}, which proves insight
into how the model performs when trained and
evaluated on different subsets of data and how generalizes.

% Explanation interpretation of variance
A low variance indicates that the model's performance is consistent across
different folds, suggesting that the model is not overfitting the training data.
Conversely, a high variance implies that the performance can vary significantly
depending on the specific data points used the test set, indicating a potential
overfitting problem.

The $R^2$ statistic is a commonly used measure for evaluating the performance of regression models.
It is used as an estimator for the trained model and is a statistical measure that represents the percentage of the
response variable variation that is explained by a linear
model~\cite[p. 43]{muller_introductionmachinelearning_2016}.
The $R^2$ score ranges between 0 and 1, where a score of 0 indicates that the model does not explain any of the
variance in the response variable around its mean, and a score of 1 indicates that the model explains all the
variance in the response variable around its mean~\cite[p. 43]{muller_introductionmachinelearning_2016}.


Equation~\eqref{eq:r2} shows the formula for the $R^2$ score:

\begin{equation}
    \label{eq:r2}
    R^2 = \frac{Explained\_variance}{Total\_variance\_targert\_variable}
\end{equation}

One unique feature of scikit-learn's implementation of $R^2$ is that the score can be negative if the model performs
poorly.
For instance, if a model always predicts the expected value of y, regardless of the input features, it would
receive an $R^2$ score of 0.0~\cite{_sklearnmetricsr2_}. Therefore, a high $R^2$ score indicates a good model fit and
a good bias-variance tradeoff~\cite[p. 43]{muller_introductionmachinelearning_2016}.

\subsection{Results}\label{subsec:results3}

Table~\ref*{tab:results_relevance} displays the variance of cross-validation and the $R^2$ for all the machine
learning models used, sorted in ascending order of their $R^2$ score. The variance of cross-validation was calculated
using Scikit-Learn's \texttt{cross\_val\_score} method, and five-fold cross-validation was used for the calculation.
The formula shown in Equation~\ref{eq:r2} was used to calculate the $R^2$ score.

It can be observed that most of the models have relatively low variance, indicating that their performance is
consistent across different folds.
Based on the table the following models have the best bias-variance tradeoff:

\begin{enumerate}
    \item Extra Trees: With an $R^2$ of 0.889 and a Variance of CV of 0.031, this model performs the best among
    the listed models. It has a high $R^2$ value, meaning it can explain a significant proportion of the
    variance in the data, and a relatively low Variance of CV, indicating stable performance across different
    subsets of the data.
    \item Multi-Layer Perceptron (MLP): The MLP model has the second-highest $R^2$ value of 0.825 and a low Variance
    of CV
    of 0.023, which implies that it can explain a substantial amount of variance in the data and perform
    consistently across different subsets.
    \item Random Forest: The Random Forest model has an $R^2$ value of 0.784 and a Variance of CV of 0.041.
    Although the model explains less variance.
\end{enumerate}

Overall, it's important to consider the cross-validation scores in conjunction with
the other other metrics to get a more complete picture of how well your model is
performing. This will be done in later sections.

\begin{table}[h]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \centering
        \begin{tabular}{lll}
            \toprule
            \thead{\textbf{Model Name}} & \thead{\textbf{Variance of CV}}
            & \thead{\textbf{$R^2$}} \\
            \toprule
            \textbf{Support Vector Machine} & 0.016 & 0.893 \\
            \hdashline
            \textbf{Extra Trees}            & 0.031 & 0.889 \\
            \hdashline
            \textbf{Multi-Layer-Perceptron} & 0.023 & 0.825 \\
            \hdashline
            \textbf{Random Forest}          & 0.041 & 0.784 \\
            \hdashline
            \textbf{Gradient Boosted Trees} & 0.021 & 0.761 \\
            \hdashline
            \textbf{Ada Boost}              & 0.072 & 0.676 \\
            \bottomrule
        \end{tabular}
    \end{tcolorbox}
    \caption{Performance of the used machine learning models according
    relevance.}
    \label{tab:results_relevance}
\end{table}


\section{DP3: Robustness}\label{sec:robustness}
% Definition of robustness

Unlike \ac{DP} Correctness(\ref{sec:dp1:-correctness}), robustness is a non-functional characteristic of a \ac{ML}
model.
A way to evaluate the robustness is to check the correctness of the model with added noise to the
data~\cite[p. 18]{zhou_machinelearning_2021}, if the model is still able to predict the correct values, it is
considered robust.
As mentioned in section~\ref{subsec:dataset-exploration} the data set
was generated in a controlled environment does not contain many data quality issues and noise as they where removed
during the process.
This offers the opportunity to test the robustness of the models by manipulating the data set to introduce the data
quality issues missing values and noise.
To do that two tests where developed and are described in the following sections.

\subsection{Missing Data}\label{subsec:missing-data}
When applying the model to real-world data, it is possible that some values are missing.
Looking at the available dataset two scenarios or missing data can occur:
Missing $Vt$-pairings and missing values.

In the first scenario, there may be no data available for a specific die opening, which can be due to the die opening
was never being used before or a lack of recorded information.
In the second scenario, while data may exist for the desired $Vt$ combination, certain values may
be missing or the dataset may not be complete.

To address these scenarios, the following tests were conducted:

\subsubsection{Missing values}

Initially, the use of the \ac{LPOCV} method was considered.
However, this approach was omitted due to computational constraints, as it generates all possible
training and test sets by removing $p$ samples from the complete dataset, resulting in a large
number of overlapping test sets and a high computational cost.
As an alternative, regular \ac{CV} was applied, with an increasing number of folds.
The minimum number of folds was 2 and the maximum was equal to the total number of samples in the
dataset.
However, this experiment did not provide distinguishable results and was therefore not
included in the results.

The third and final experiment utilized random sampling to create different train- test data compositions, allowing
for the evaluation of the model's handling of missing data.
Split rations from 90:10 to 10:90 were used, with a step size of 10.
This approach was inspired by a similar method used in a previous study~\cite[p. 570--574]{liu2021deep}  which used
thee different split ratios to evaluate the performance of a deep learning model.

The results of this experiment provided valuable insights into the model's performance under
different data compositions, particularly when dealing with missing
data, and were used in conjunction with other metrics to gain a comprehensive understanding of
the model's overall performance.

Figure~\ref{fig:results-missing-values} shows how the models performed when trained on
continuously less data.
It can be seen that all models perform well when trained with on 90\% of the data.
However, when trained on 50\% of the data, the \ac{MLP}  and \ac{GBT} models start to perform
better than the
other models.
When trained on 20\% of the data and less all models significantly underperform.
Also the \ac{ET} model stays relatively stable when trained on less data.

\begin{figure}[H]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \centering
        \includegraphics[width=0.6\textwidth]{chap5/images/missing_values_plot}
    \end{tcolorbox}
    \caption{Comparison of performance on less training data.}
    \label{fig:results-missing-values}
\end{figure}

Looking at the variances show in Figure~\ref{fig:variance-missing-values} the the \ac{SVM} and \ac{MLP} models
perform the and seem to be the most robust models when trained on less data.
It has to be noted, that as seen in the previous figure the most varuance comes from training on
below 20\% of the data.

Overall the \ac{MLP} model seems to be the most robust model, as it performs well on all data
compositions and has the lowest variance.

\begin{figure}[h]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \centering
        \includegraphics[width=0.6\textwidth]{chap5/images/variance_missing_values}
    \end{tcolorbox}
    \caption{Variance of models when trained on different data.}
    \label{fig:variance-missing-values}
\end{figure}

\subsection{Noise}\label{subsec:noise}
A common practise in \ac{ML} is to add noise to the training data to improve
the model's generalization abilities.
In the following section Gaussian Noise will be added to the data set to evaluate the robustness
of the model.
The noise will be added to the training data only as the training data should be as correct as possible to get
comparable results.

To add the noise, \textit{numpy.random.normal} function was used, which
generate random numbers with a Gaussian distribution~\cite{scikit-learn}.
The mean and standard deviations of each feature were calculated based on the original data, and the function was
used to generate noisy values for each feature.

To study the model's reaction to increasing amounts of noise, the noise added
to the training data was gradually increased starting from 1\% to 50\%, which means that in the
last iteration contains as many noisy as `clean' samples.
The difference between all \ac{RMSE} between iterations was defined as loss.
The total loss was calculated by averaging the difference of these values as shown in Equation~\ref{eq:noise}.

\begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
    \begin{equation}
        \text{Average Loss} = \frac{1}{N} \sum_{i=1}^{N} |\text{RMSE}i -
        \text{RMSE}{avg}|\label
        {eq:noise}
    \end{equation}
\end{tcolorbox}

\begin{itemize}
    \item \textit{Does the calculation of the loss make sense?}
\end{itemize}

\subsection{Results}\label{subsec:results-robustness}


Figure~\ref{fig:results-noise-fig} presents the performance of models as more noise is introduced to the training
dataset. The noisy dataset comprised 1000 samples, and the x-axis indicates the percentage of the original data that
was augmented with noise. The dashed lines correspond to the default performance of the models in the absence of noise.

The results indicate that the performance of all models deteriorates rapidly with the addition of noise.
Specifically, the first 20\% of the noisy data causes the most significant decline in model performance, after which the
performance plateaus at a poor level.
It is noteworthy that the Extra Trees model exhibits the most robust performance, outperforming the Gradient Boosted
Trees and Random Forest models, which also demonstrate similar levels of resilience.

The results are interesting since they demonstrate that the typically high-performing SVM and MLP models
exhibit underperformance under the current conditions.
Specifically, the MLP is a highly expressive model with many parameters, making it prone to overfitting.
In this case, the addition of noise is likely causing the model to overfit to the noise, leading to poor
generalization performance.
On the other hand, the SVM aims to find an optimal hyperplane to separate the data with maximum margin, which makes
it highly sensitive to outliers and noise, making it challenging to identify the appropriate hyperplane.

These results emphasize the significance of selecting robust models and pre-processing the data to minimize the
presence of outliers.
Moreover, the study reveals that even a small amount of noise (i.e., 10\%) can significantly impact the model's
performance.

\begin{figure}[h]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \centering
        \includegraphics[width=0.9\textwidth]{chap5/images/results_noise}
    \end{tcolorbox}
    \caption{Comparison of performance on noisy data.
    The x-axis shows how many percent of the noisy dataset where added to the training data.
    The y-axis shows the MSE metric.
    The dotted lines show the default performance of the model.
    The solid lines show the performance of the model when noise was added to the training data.
    }
    \label{fig:results-noise-fig}
\end{figure}


\section{DP4: Stability}\label{sec:stability}

To evaluate the model's stability, the \ac{LOOCV} was repeated for all samples in the dataset, resulting in a total
of $n$ iterations. The stability of the model was then determined by calculating the average prediction error across
all iterations, using the equation provided in Equation~\ref{eq:loocv}~\cite[p. 201]{gareth2013introduction}.

\begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
    \begin{equation}
        CV_{(n)} = \frac{1}{n} \sum_{i=1}^{n} \text{MSE}_{i}\label{eq:loocv}
    \end{equation}
\end{tcolorbox}

However, it is important to note that the resulting mean squared error (\ac{MSE}) is a poor estimate of the model's
generalization error since only one sample is used for validation~\cite[p. 201]{gareth2013introduction}.
Nevertheless, it can still be used to assess the model's stability, which is the primary focus of this section.
The stability of the model can be evaluated by examining the standard deviation of the cross-validation scores.
A low standard deviation across all folds indicates that the model can generate consistent results when trained on
different
datasets, which suggests a more stable model.

It is worth noting that there are several ways to measure the stability of a model. In this study, \ac{LOOCV} was
chosen for several reasons.
First, \ac{LOOCV} uses almost all the available data for training, while k-fold cross -validation reserves a portion
of the data for testing.
Since the dataset used in this study is relatively small,using all the data for training is advantageous.
Second, \ac{LOOCV} typically has lower bias due to the larger training set.
Lastly, various approaches were attempted to measure the model's stability, and \ac{LOOCV} delivered
the most stable and interpretable results.
The primary drawback of this method is its computational expense since it
requires $n$ iterations.

Table~\ref{subsec:results-stability} presents the cross-validation score and standard deviation for each model. The
focus of the analysis was on the standard deviation since it provides a better indication of the model's stability.
As mentioned earlier, the \ac{CV} score using \ac{LOOCV} is a poor estimator of the model's generalization error.

\subsection{Results}\label{subsec:results-stability}

The Table~\ref{tab:results-stability} show the mean cross-validation score and standard deviation for each model.
In order to evaluate the stability of the model, the standard deviation was used as the primary metric.

The stability analysis using LOOCV reveals that the Gradient Boosting model is the most stable with a standard
deviation of 0.183.
The Etra Trees and Random Forest model exhibit similar levels of stability with mean CV scores of 0.047 and 0.049,
and standard deviations of 0.206 and 0.194
It can be observed again, similarar to the evaluation of the robustness that the MLP and SVM models exhibit
lower stability levels compared to the other models even though they where the most promising in the evaluation of
the correctness.

While stability is an important factor to consider, other aspects in particular the generalization error (correctness)
should be also taken into account.

\begin{table}[h]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
% \sisetup{group-minimum-digits = 4}
        \centering
        \begin{tabular}{ll}
            \toprule
            \thead{\textbf{Model Name}} & \thead{\textbf{mean cv score ± std}}
            \\
            \toprule
            \textbf{Gradient Boosting}      & 0.038 ± 0.183 \\
            \hdashline
            \textbf{Random Forest}          & 0.047 ± 0.206 \\
            \hdashline
            \textbf{Extra Trees}            & 0.049 ± 0.194 \\
            \hdashline
            \textbf{Support Vector Machine} & 0.053 ± 0.244 \\
            \hdashline
            \textbf{MLP}                    & 0.053 ± 0.244 \\
            \hdashline
            \bottomrule
        \end{tabular}
    \end{tcolorbox}
    \caption{Results of the stability of the models.}
    \label{tab:results-stability}
\end{table}


\section{DP5: Resource utilization}\label{sec:resource-utilization}

To evaluate the resource utilization of the model, several metrics can be employed.
As discussed in Section~\ref{subsec:dp5-resource-utilization}, this study concentrates on time, computational power,
and memory resources.
Consequently, the metrics utilized to assess resource utilization include training time, inference time, and memory
usage.

The training time is measured in seconds and refers to the time that is required to train the model.
Training a model need computational resources such as memory and CPU power, thus the longer the training process
takes the more resources are needed.
Therefore a shorter training time is preferred.
To measure the training time the python function \texttt{time.time} is employed.
The functions does return the time in seconds since the epoch.
The time is recorded when the model is fitted and the difference between the two values
represents the training time.

The inference time is measured in milliseconds and refers to the time the models need to make predictions.
This metric is important for real-world use cases because user do not want to wait long for predictions and the
process also needs computational resources.
Since most models are able to make one predictions quite fast the inference time is measured for 100 predictions
using the \texttt{time.time} function again.

The memory usage is measured in megabytes (MB) and refers to the amount of memory that is required to run and save
the mode.
A smaller memory footprint is preferred because it required less resources in the form of storage space.
To measure the memory usage the \texttt{memory\_usage} function from the package \texttt{memory\_profiler} is
employed.

\subsection*{Results}

Table~\ref{tab:results_resource_utilization} shows the results for the text that where made.
Looking at the traing time the Extra Trees and Random Forest model perform the with the shortes training times
of 19.541 and 24.912 ms
The Support Vector Maschine and Gradient boosted model have intermediate training times.
The MLP is clearly the slowest with over 16 seconds.
The training time can be attributed the nature of the models and their training algrithm.
The RF and ET both benefit from the bagging technique which means that they train an ensemble of decision trees and
combine them to one big model.
This can be done in parallel and therefore massivly speeds up the training
process (see Section~\ref{subsubsec:random -forests}).
On the other side MLPs are typically more computational since feeding forward and backpropagation are not
parallelize and computationally expensive in general (see Section~\ref{subsec:neural-networks}).

A comparable picture can be derived for the inference time.
This time the GBR, which is also a tree based algithm is the fasted to make predictions while the MLP is the slowest.
The ET, RF and SVM models have comparable times.

Looking at the memory usage the models are quite similar with the MLP using the most memory 170 KB.
The other models have quite similar memory usage raning from 157 to 158 KB.
Overall the memory space seems not to be a big issue for the models, a reason for that is the fact that the
dataset is not very big and the resuling models are also less complex.


\begin{table}[h]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \centering
        \begin{tabular}{llll}
            \toprule
            \thead{\textbf{Model Name}} & {\thead{\textbf{Training time} \\
            \unit[]{ms}}}
            & {\thead{\textbf{Inference time} \\ \unit[]{ms}}} &
                {\thead{\textbf{Memory Usage} \\
            \unit{kb}}}
            \\
            \toprule
            \textbf{ET}  & 19.541     & 1.939  & 157.93 \\
            \hdashline
            \textbf{RF} & 24.912     & 2.027  & 157.684 \\
            \hdashline
            \textbf{GBR} & 43.688    & 1.302  & 158.121 \\
            \hdashline
            \textbf{SVM} & 421.007 & 2.587 & 158.082 \\
            \hdashline
            \textbf{MLP} & 16466.261s & 17.988 & 170.715 \\
            \bottomrule
        \end{tabular}
    \end{tcolorbox}
    \caption{Overview of the used machine learning models and their metrics.}
    \label{tab:results_resource_utilization}
\end{table}


\section{DP6: Interpretability}\label{sec:interpretability}
In accordance with the definitions of interpretability outlined in Section~\ref{sec:objectives-of-a-solution}, it
becomes apparent that quantitatively measuring interpretability is not viable.
However, alternative approaches can be employed to gauge a model's interpretability.
\cite{molnar2020interpretable} distinguishes between model-agnostic and model-specific methods.
Model-agnostic methods can be applied to any model, whereas model-specific methods are tailored to a particular
model~\cite[p. 19]{molnar2020interpretable}.

\cite{molnar2020interpretable} differentiates between model-agnostic and model-specific methods.
Model-agnostic methods are methods that can be applied to any model, while model-specific methods are
methods that are specific to a model~\cite[p. 19]{molnar2020interpretable}.

One strategy for achieving interpretability is restricting the selection of algorithms to those that yield
model specific interpretable outcomes.
Interpretable algorithms used in this study are linear regression and Decision
Trees~\cite[p. 35]{molnar2020interpretable}.
Nonetheless, as discussed in Section~\ref{sec:dp1:-correctness}, these algorithms do not demonstrate sufficient
performance to be deemed viable solutions for predicting spring back.
Consequently, this study cannot rely on model-specific interpretability methods and should instead utilize model
-agnostic methods for generating explanations.

Also~\cite{molnar2020interpretable} differentiates between global and local interpretability.
Global interpretability refers to the ability to understand the overall behavior of
the model, while local interpretability refers to the ability to understand the behavior of the
model for a specific instance.

It does not make sense to apply model-agnostic method on all trained models, it makes more sense to
apply the methods on the best performing models.
In the other parts of the evaluation, the Random Forest, Support Vector Machine and Multi Layer Perceptron models
where always under the best perfoming models and are therefore the models that are used in the following
sections.

\subsection{Global Model-Agnostic Methods}\label{subsec:global-model-agnostic-methods}
In the following section, global model-agnostic methods are applied to the trained machine learning models to
understand their overall behavior.
Two commonly used methods for analyzing the relationship between input features
and the target variable are Feature Dependence plots and Partial Dependence plots.
Feature dependence plots illustrate the relationship between a feature and the target variable.
By analyzing these plots, we can identify which features have the strongest impact on the model's
predictions, making it easier to prioritize and refine our feature selection.

In the case of \ac{MLP}, there is no built-in feature importance, as the importance of each feature is determined by
the weights assigned to the perceptron.
For \ac{SVM}, feature importance cannot be directly derived from the model
because the algorithm applies a hyperplane that separates the data.
The importance of a feature depends on the
influence of the hyperplane, which is not easily measurable.
Consequently, feature importance cannot be plotted for
the two chosen models.
In contrast, other models, such as random forests, have built-in feature importance that can
be utilized to analyze the model.


Figure~\ref{fig:feature_importances_rf} compares and visualizes the relative importance of the features used for
training the model.
The Partial Dependence Plots (PDP) provide insights into the relevance and interactions of the
features.
As illustrated, thickness is the most important feature, followed by distance and die opening.
This means
that changes in thickness have the most significant impact on spring back, while variations in the other two features
have a comparatively smaller influence on the outcome.
In terms of interactions, all features are relevant, indicating that each feature contributes unique information to
the model.
This could suggest that the features are not highly correlated with each other, as their interactions
could already be observed in the correlation matrix shown in Figure~\ref{fig:correlation_matrix}.
Consequently, removing one feature would result in a significant loss of information and, therefore, poorer
performance of the model.

\begin{figure}[h]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \centering
        \includegraphics[width=0.7\textwidth]{chap5/images/rf_feature_importances}
    \end{tcolorbox}
    \caption{Feature importance of the random forest model.}
    \label{fig:feature-importances-rf}
\end{figure}

The feature importance plot did facilitate a better understanding of the relationship between the features and the
target
variable.
However, they do not provide insights into exactly how the features influence the outcome. Partial dependence plots
can be helpful for this purpose.

\subsubsection{Partial Dependence}
\cite{greenwell2018simple} introduced another feature importance measure based on partial dependence.
Partial dependence refers to the relationship between a target variable and a single predictor variable in a statistical
model, while keeping all other predictor variables constant.
A feature that exhibits a consistent partial dependence across all values is less important than a feature that

Partial dependence plots examine the impact of a single input feature on the target variable while keeping all other
features constant.
While the feature importance plot shows the relative importance of each feature, the partial dependence plot shows
the impact of a single feature on the target variable.

In Figure~\ref{fig:partial_dependence_plots}, we can see the PDPs for two models: Support Vector Machines (SVM) and
Multilayer Perceptron (MLP).
It can be seen that the plots for both models yield the same results for the three features.
The results show that a higher $y_p$ value leads to a greater spring back, while
increasing the thickness of the metal sheet results in a lower spring back.
The effect of the die opening is less
significant than the other two features, but there is still a noticeable increase in spring back with higher die
opening.
The PDPs demonstrate that all features exhibit a non-linear relationship with the target variable.
This information
can be useful for understanding the behavior of the models and the impact of different features on the predicted
outcome.

\begin{figure}[h]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \centering
        \begin{subfigure}{0.45\textwidth}
            \includegraphics[width=\textwidth]{chap5/images/partial_dependence_SVM}
            \caption{SVM}
            \label{fig:feature_impoartances_rf}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.45\textwidth}
            \includegraphics[width=\textwidth]{chap5/images/partial_dependence_MLP}
            \caption{MLP}
            \label{fig:partial_dependence_svm}
        \end{subfigure}
        \hfill
        \caption{Partial dependence plots}

        \label{fig:partial_dependence_plots}
    \end{tcolorbox}
\end{figure}

It is important to note that partial dependence plots and feature importance plots are limited to
identifying linear relationships between features and the target variable.
Since this is not the case as state previously, the results of the partial dependence plots are
alternative models as permutation feature importance and SHAP values are used to gain further
information.

\subsection{Permutation Feature Importance and SHAP}\label{subsec:permutation-feature-importance-
and-shap}

Permutation feature importance's assesses the impact on the model's prediction error when the values
of a feature a randomly permutes.
Permutation in this context means the process of randomly shuffling the values of a single features while keeping the
values of all other feautres unchanged.
Thereby the association between the feature and the actual outcome is disrupted~\cite[p. 157]{molnar2020interpretable}.

The importance of a feature is evaluated by assessing the degree to which permuting its values
results in an increase in the prediction error fo the model.
If permuting a features results in a significant increases in errors the features is deemed
important because the model relied on it for making accurate predictions.
Conversely, if permuting a feature doesn't change the prediction error is considered unimportant
because the model didn't utilize it for its prediction~\cite[p. 158]{molnar2020interpretable}.

The resutls presented in Figure~\ref{fig:permutation_feature_importance} show that hte MLP and the SVM models yield
similar trends in the importance of features.
The feautres thickness and punch penetrration have a high importance with their permutation causing a significant
increase
in the prediction error both in $R^2$ and $MSE$.
This inidicates that they are crucial for the decision making of the model.

Different from the previous plots, the die opening has a importance of zero in both models.
Ths implies does permuting this features does not have any influence on the prediction error.
There are two possible explanations for this.

1. The die opening is not relevant for the decision making and does not provide any significant information.

2. The die opening is important but only in combination with either the thickness or the penetration distance.
This suggest that there might be an interactions effect that was not captures in previous analysis.

Since the normal feature importance plots showed that die opening is important the second explanation is more likely.
To further investigate this possiblitly a partial dependence plot was created for the die opening in combination with
either the thickness or the penetration distance.

\begin{table}[h]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \centering
        \subfloat[Subtable 1 list of tables text][MLP]{
            \begin{tabular}{lll}
                \toprule
                \multicolumn{3}{}{\textbf{$R^2$}} \\
                \toprule
                thickness   & 1.337 & +/- 0.173542 \\
                \hdashline
                distance    & 1.293 & +/- 0.145488 \\
                \hdashline
                die opening & 0.000 & +/- 0.000000 \\
                \toprule
                \multicolumn{3}{l}{\textbf{$MSE$}} \\
                \toprule
                thickness   & 0.274 & +/- 0.035613 \\
                \hdashline
                distance    & 0.265 & +/- 0.029856 \\
                \hdashline
                die opening & 0.000 & +/- 0.000000 \\
                \bottomline
            \end{tabular}}
        \qquad
        \subfloat[Subtable 2 list of tables text][SVM]{
            \begin{tabular}{lll}
                \toprule
                \multicolumn{3}{}{\textbf{$R^2$}} \\
                \toprule
                distance    & 1.237 & +/- 0.139285 \\
                \hdashline
                thickness   & 1.178 & +/- 0.157247 \\
                \hdashline
                die opening & 0.000 & +/- 0.000000 \\
                \toprule
                \multicolumn{3}{l}{\textbf{$MSE$}} \\
                \toprule
                distance    & 0.254 & +/- 0.028583 \\
                \hdashline
                thickness   & 0.242 & +/- 0.032269 \\
                \hdashline
                die opening & 0.000 & +/- 0.000000 \\
                \bottomline
            \end{tabular}}
    \end{tcolorbox}
    \caption{Permutation feature importance}
    \label{tab:permutation_feature_importance}
\end{table}

To investigate the joint performance of the die opening feature with the other two features,
again partial dependence plots can be used.
Figure~\ref{fig:partial_dependence_plots} shows a two-way partial dependence plot for the
\ac{MLP} model.
The x- and y-axes represent the thickness and the distance, respectively.
The slope of the contour lines indicates the direction of the effect of the die opening on the
prediction.
The contour lines are colored and labeled according to the value of the prediction.
It can be seen that the contour lines are not parallel which suggest the two features are
interaction with each other and therefore have a joint effect on the model prediction.
This means that the second explanation mentioned above is correct and the die opening is not irrelevant for the model
prediction but just interacts with the other two features.

\begin{figure}[h]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \centering
        \includegraphics[width=0.8\textwidth]{chap5/images/pdp_distance_die_opening}
    \end{tcolorbox}
    \caption{DPD distance and die opening}
    \label{fig:dpd-distance-die-opening}
\end{figure}

\subsection{Local Model Agnostic Methods}\label{subsec:local-model-agnostic-methods}
Local interpretability refers to the ability to understand the behavior of the model for a
specific instance and is often used to explain the predictions of a model.
Model agnostic methods can be used to explain the predictions of any model type~\cite{molnar2020interpretable}.

Local methods rely on interpreting individual instances from a dataset, and as such, it is
crucial to select representative samples.
One way to achieve this is by considering the V/t ratio of each instance.

\subsection{LIME}\label{subsec:lime}
The LIME method stands for Local interpretable model-agnostic explanations
and a model itself that can be used to explain the predictions of any machine
learning model, the model is treated as a black-box~\cite{ribeiro2016model}.
To accomplish this, LIME generates a new dataset with permuted samples along with the predictions
made by the black box model for each sample.
Using this dataset, LIME trains an interpretable model that is given weights based on the
proximity of the sampled instance to the instance of interest.
The interpretable model can be any model that is considered interpretable, such as a Liner
Regression or Decision Trees.

This approach allows LIME to provide explanations for individual predictions by approximating the
behavior of the black box model in a localized and interpretable way~\cite[p. 185]{molnar2020interpretable}.

\cite{molnar2020interpretable} express local surrogate models like LIME as follows:

\begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
    \begin{equation}
        explanation(x) = arg\; min\underset{g \in G}\; L(f,g,\pi_x) + \Omega(g)
    \end{equation}
\end{tcolorbox}

The model for explaining a certain instance $x$ is represented by the model $g$ (such as a
linear regression model), which reduces the loss (for example \ac{MSE}), measuring how closely
the explanation aligns with the prediction made by the original model $f$ (for instance, an
\ac{MLP} model).
At the same time, the model complexity $\Omega(g)$ should remain low.
The set of all possible linear regression models is referred to as $G$ , which encompasses all
potential explanations.
The proximity measure, denoted as $\pi_x$ , determines the size of the area surrounding the area
surrounding the instance $x$ that is taken into account for the
explanation~\cite[p. 185]{molnar2020interpretable}.

LIME mainly provides instance explanations in the form of feature importance values.
It can show the predicted values along with the explanations on how which features contributed
in which direction to the prediction.
This can be seen similar to PDP plots but instead of showing the average effect of a feature
on the overall prediction of the model, it shows the effect of a feature on the prediction
for a specific instance.
One instance in this case consists of the thickness, distance die opening and the corresponding
spring back value.


\textit{If I have questions for specific instances I should put them here.}


\section{Demonstration}\label{sec:demonstration}
This section represents the demonstration activity in the \ac{DSR} process.
This activity focuses on the demonstration of the developed models in specific selected scenarios and not the
overall performance like in the evaluation activity.
For this purpose three scenarios are selected and the models are demonstrated in these scenarios.

For the current dataset, the V/t ratios range from 10 to 100. The highest ratio can be achieved by bending a 0.5 mm
thin metal sheet using a 50 mm wide die opening, while the lowest ratio is
achieved by bending a 1 mm thick sheet on a 10 mm die opening.

Recommended V/t ratios in industrial practices are between 6 and 10~\cite[p.7]{cruz_applicationmachinelearning_2021}.
Bending operations performed outside of this range of recommended ratios may result in
high spring back.
Through testing and a look at the dataset the optimal V/t ratios for the experimental setups used in this study is
different from the recommended V/t ratios.
As seen in Section~\ref{subsec:training-test-split} `Test-Train Split' it was not possible to squeeze metals larger
than 1 mm through a die opening of 10 mm.
This means that the lowest possible V/t ratio is 10 and the industrial practise of 6 to 10 is not exactly applicable
for the experimental setup used in this study.

A look a the mean springbacks can help here.
Figure~\ref{fig:springback-heatmap} shows the mean spring back for each V/t ratio.
It can be seen, that V/t of 100 results by far in the highest spring back while a V/t of 10 results in the lowest.

\begin{figure}[h]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \centering
        \includegraphics[width=0.9\textwidth]{chap5/images/mean_springback_heatmap}
    \end{tcolorbox}
    \caption{Feature importance of the random forest model.}
    \label{fig:springback-heatmap}
\end{figure}


The selected representative instances for the local methods are from one of three cases:
\begin{itemize}
    \item Case A includes instances with V/t ratios below the recommended range.
    \item Case B includes instances within the recommended range of V/t ratios.
    \item Case C includes instances with V/t ratios above the recommended range.
\end{itemize}

Chosen instances from each case are shown in Table~\ref{tab:representative-instances}.

\begin{table}[H]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \centering
        \begin{tabular}{lllll}
            \toprule
            \textbf{Case} & \textbf{\(V\) } & \textbf{\(t\)} & \textbf{\(y_p\)} & \textbf{V/t} \\
            \toprule
            A             & 10              & 1.0            & ?                & 10           \\
            A             & 20              & 3.0            & ?                & 6.66         \\
            \hdashline
            B             & 30              & 1.5            & ?                & 20           \\
            B             & 20              & 1.5            & ?                & 13.33        \\
            \hdashline
            C             & 50              & 1              & ?                & 50           \\
            C             & 50              & 0.5            & ?                & 100          \\
            \bottomrule
        \end{tabular}
    \end{tcolorbox}
    \caption{Representative instances for the local methods.}
    \label{tab:representative-instances}
\end{table}

\subsection{Comparison of the Models}\label{subsec:overall-comparison-model-performance}
This section will showcase the outcomes generated by the trained models in more detail and
will also discuss the results in relation to the research questions.
This is done in this section to find out the best perfoming models.
These models will be researched in more detail later with the help of model-agnostic
methods to explain the predictions of the models.

In order to evaluate the model's performance without bias, each test case where removed
from the training data set so that the model has not seen the test case before.
This results in a small but notable performance decrease in the test cases, but makes sure that the
results are not biased.

For the interpretation of the next picutres it is relevant to nkow that the target values
repesent the man of all values.
The quality the dataset is but not perfect, and it is possible that there are some outliers in the
dataset.

Figure~\ref{fig:performance-case-a} shows show two chosen test cases for case A with a
V/t-ratio of 20 (\ref{fig:performance-20-1} and 6.6 (\ref{fig:performance-20-3}.

Case~\ref{fig:performance-20-1} with a V/t-ratio of 20 demonstrates the models'
ability to predict the spring with high accuracy.
However, the performance decreases in case b)\ref{fig:performance-20-3} with a V/t-ratio
of 6.6, particularly when predicting the spring backs.
It is likely that there is a measurement error at V = 7.5 in case
b)~\ref{fig:performance-20-3}, causing some models to potentially predict the correct
spring back.

\begin{figure}[H]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \begin{subfigure}{0.5\textwidth}
            \includegraphics[width=\textwidth]{chap5/images/performance_20_3}
            \caption{V: 20, t: 1}
            \label{fig:performance-20-3-1}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.5\textwidth}
            \includegraphics[width=\textwidth]{chap5/images/performance_20_3}
            \caption{V: 20, t: 3}
            \label{fig:performance-20-3-2}
        \end{subfigure}
    \end{tcolorbox}
    \caption{Performance plots for case A}
    \label{fig:performance-case-a}
\end{figure}


Figure~\ref{fig:performance-case-b} displays two selected test series for case B, which indicates
that the V/t ratios fall within the recommended range. These test series produced the best
results compared to cases A and C.

As depicted in figure~\ref{fig:performance-20-1}, both models accurately predicted the spring
back with high precision. The \ac{RF}, \ac{SVM}, and \ac{MLP} models demonstrated similar
performance and produced usable results. Similar findings were observed for figure~\ref{fig
:performance-30_2.0}, where the V/t ratio was different. In this case, the \ac{MLP} model
performed well for \(y_p\) values of 5 and 10, but became inaccurate for higher values. On the
other hand, the \ac{SLP} model consistently produced better results, while the \ac{ET} model
still struggled to perform well.

Overall, it is challenging to identify a clear favorite model based on the performance of the
models in these test series.
While some models performed better than others for specific variables or scenarios, the
differences were not significant enough to identify a clear winner.
Therefore, selecting an appropriate model will be decided by the other two cases A and C.

\begin{figure}[H]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \begin{subfigure}{0.5\textwidth}
            \includegraphics[width=\textwidth]{chap5/images/performance_20_1}
            \caption{V: 20, t: 1}
            \label{fig:performance-20-1}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.5\textwidth}
            \includegraphics[width=\textwidth]{chap5/images/performance_30_2}
            \caption{V: 30, t: 2}
            \label{fig:performance-30_2.0}
        \end{subfigure}
    \end{tcolorbox}
    \caption{Performance plots for case B}
    \label{fig:performance-case-b}
\end{figure}

Case C is the most challenging case, as the V/t ratios are not within the recommended range.
Figure~\ref{fig:performance-case-c} shows the performance of the models for two test series
with V/t ratios of 50 and 100.

The results shown in figure~\ref{fig:performance-50_0.5} are clear.
The results are unusable, as the models are not able to predict the spring back.
In figure ~\ref{fig:performance-50_1} the results look better, the \ac{RF} and \ac{SVM} model
seem to be the most consistent models?

\begin{figure}[H]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \begin{subfigure}{0.5\textwidth}
            \includegraphics[width=\textwidth]{chap5/images/performance_50_0.5}
            \caption{V: 50, t: 0.5}
            \label{fig:performance-50_0.5}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.5\textwidth}
            \includegraphics[width=\textwidth]{chap5/images/performance_50_1}
            \caption{V: 50, t: 1}
            \label{fig:performance-50_1}
        \end{subfigure}
    \end{tcolorbox}
    \caption{Performance plots for case C}
    \label{fig:performance-case-c}
\end{figure}

The best overall performing models are the \ac{MLP} and the \ac{SVM} model.
As seen before, the model performs consistent across all thee scenarios, indicating that it can
effectively handle a broad range of V/t ratios also outside the recommended industry guidelines
range.
Therefore the chosen models for more detailed analysis are the \ac{MLP} and the \ac{SVM} model.

Overall it can be said that the models are able to predict the spring back with high accuracy.
For bad predictions the error is less than 0.25 mm, which is within the tolerance of the
most manufacturing processes (This it not true yet).