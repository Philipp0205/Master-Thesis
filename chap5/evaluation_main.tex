\chapter{Evaluation}\label{ch:evaluation}
% take a step back and put your results from 4 into context.
This chapter represents the evaluation phase, which includes the evaluation, demonstration and
communication activities outlined in the \ac{DSR} process in \cref{ch:research-methodology}.

The evaluation activity provides a comprehensive analysis of the Machine Learning models built in the
previous chapter.
This phase evaluates the effectiveness of the developed models in solving the defined research questions.
During the demonstration phase, different scenarios are chosen to illustrate the potential real-world applications of
the models and their performance in these contexts.
Finally, during the communication phase, the results of this study will be presented with the aim of effectively
communicate the results and their interpretation.

\cref{tab:evaluation_criteria} provides an overview of the quality metrics employed for the evaluation,
structured based on the \ac{GQM} approach discussed in \cref{sec:goal-question-metric-approach}.
The quality metrics align with the quality model from~\cite{siebert2022construction}, which in turn is based on
the ISO/IEC 9126 standard for software quality evaluation.
This standard has been adapted by~\cite{siebert2022construction} to address the specific requirements of machine
learning models, as detailed in Section~\ref{sec:evaluation-of-machine-learning-models}.
Since~\cite{siebert2022construction} primarily focus on evaluating classification models, the quality metrics have
been tailored to suit the needs of the regression models implemented in this study.
Each individual quality metric section provides further elaboration, on what was changed.
\cref{tab:evaluation_criteria} also displays the quality metrics used for evaluating the \ac{ML} models.

\captionsetup{margin={5pt,5pt}}
\begin{table}[H]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \centering
        {\renewcommand{\arraystretch}{1}
            \begin{tabular}{p{2cm}p{8cm}p{3cm}}
                \toprule
                \thead{\textbf{Goal}} & \thead{\textbf{Question}}
                & \thead{\textbf{Metric}} \\
                \toprule
                \textbf{Correctness} &
                How is the ability of the model to perform the current task measured on the development dataset and
                the runtime dataset
                ~\cite[p. 16]{siebert2022construction}?
                &
                MAE, \newline MSE, \newline RMSE
                \\
                \hdashline
                \textbf{Relevance} &
                Does the bias-variance tradeoff work well for the model?
                Which means that the data should not be overfit or underfit.
                ~\cite[p. 16]{siebert2022construction}?
                & Variance of CV, \newline $R^2$
                \\
                \hdashline
                \textbf{Robustness} & Ability of the model to outliers, noise
                and other data quality issues~\cite[p. 16]{siebert2022construction}
                & Loss of Accuracy, \newline Average Loss
                \\
                \hdashline
                \textbf{Stability} & Does the artifact generate repeatable
                results when trained on different data?~\cite[p. 16]{siebert2022construction}
                & LOOCV stability
                \\
                \hdashline
                \textbf{Interpret-ability} & How well can the model be
                explained?~\cite[p. 16]{siebert2022construction}
                & Different interpretability methods
                \\
                \hdashline
                \textbf{Resource utilization} & How much resources are
                required to train and run
                the model?~\cite[p. 16]{siebert2022construction}
                & Training time, \newline runtime, \newline storage space
                \\
                \bottomrule
            \end{tabular}
        } % renew command
    \end{tcolorbox}
    \caption{Overview of the goals, questions and metrics for the
    evaluation of artifacts
    following the \ac{GQM} approach.}
    \label{tab:evaluation_criteria}
\end{table}


\section{DP1: Correctness}\label{sec:dp1:-correctness}
% Ability of the model to perform the current task measured on the
% development dataset and the
% runtime dataset

The model must be able to perform well on the selected task.
\cite{siebert2022construction} used the classification metrics precision, recall and F- score to evaluate the
correctness.
Since predicting the spring back is a regression task, these metrics are not applicable.
For this study metrics are needed to measure how well the predicted spring back for a specific observation
matches the actual spring back~\cite[p. 29]{hastie2009elements}.
These metrics used in this study are the MAE, MSE and RMSE and where presented in~\cref{subsubsec:regression-metrics}
For the evaluation the \textit{sci-kit learn}~(\cite{scikit-learn}) implementations of these metrics were used.

There is an ongoing debate regarding the best metric to use for evaluating regression models.
For instance~\cite{willmott2005advantages} contend that the RMSE is not an appropriate measure for determining the
average performance of a model and suggest using the MAE instead, while~\cite{chai2014root} argue in favor of the RMSE.
The MAE is commonly used when a dataset contains outliers.
However, as the dataset reviewed in \cref{subsec:dataset-exploration} is likely to not contain many outliers, the
RMSE will be used as the primary metric in this study~\cite[p. 1249]{chai2014root}.
The MSE and MAE will be used as well but only when they make sense, for example when noise is added to the dataset
and therefore more outliers (\cref{sec:robustness}) are present or if the non squared error yields more meaningful
results.

Both measures will be included in the analysis, to give the reader a better understanding of the performance of
the models.
The MSE was not included in the analysis, as it is not expressed in the same unit as the data and is therefore
more difficult to interpret and might be misleading.

\subsection*{How much error is acceptable?}

An important question to ask when evaluating the performance of a model is how much error is acceptable.
This is a domain specific question and therefore tolerances in the domain of sheet metal forming are used.
The DIN 2768 norm provides different mechanical precision levels and tolerances, Table three describes the tolerances
for angular dimensions formed metal components.
The norm does also differentiate between different levels of precision, the highest precision level is f (fine)
followed by m (medium), c (coarse) and v (very coarse).
The norm does differentiate between parts of different side length, since the metal sheets used in this study are 80
mm long and are bent in the middle the side length is 40 mm each.
For metal parts between 10~mm and 50~mm side length the fine and medium tolerance is ±0°30'minutes and degrees
This translates to a ±0.5° tolerance of the opening angle.

That means the \ac{ML} models should be able to predict the spring back accurate enough so that the bended angle of
the metal part is within the tolerance of ±0.5°.
Since the spring back is measured in millimeters in the dataset in a first step it has to be converted to degrees to
apply the tolerance.

\subsection{Results}\label{subsec:results}
\cref{tab:results-correctness} presents the results of the evaluation of the Machine Learning models for
correctness, sorted by the RMSE in ascending order.
The results where calculated using 5-fold cross validation and the mean of the RMSE and MAE is presented.
It can be can be concluded that the best models have an RMSE of less than 0.2 millimetres, indicating that they can
predict the spring back with acceptable accuracy for many applications.

The Linear Regression and Decision Tree models have poor predictive performance compared to the other models.
For the \ac{LR} model this is expected since the relationship between the independent and dependent variables is
not linear.
In contrast, Decision Trees can perform poorly when overfitting the training data and they are prone to
instability.
These problems are mitigated by using ensemble methods like Random Forest and Gradient Boosted Trees.
As the simple Decision Tree and Linear Regression models are not able to predict the spring back accurately enough,
they are not considered for the next steps of the evaluation.
As there are only two algorithms that are deemed interpretable with explanations specific to their respective models
, it follows that alternative techniques must be identified for the interpretability assessment to be conducted later.
This matter will be deliberated in~\cref{sec:interpretability}.

The Ada Boost, Random Forest, and Gradient Boosted Tree models demonstrate moderate predictive
performance while the Multi-Layer-Perceptron, Extra Trees, and Support Vector Machine models have relatively low MAE
and RMSE values, indicating good predictive performance.
It is worth noting that the SVM model has a slightly better MAE while still maintaining a comparable RMSE compared
to the other models.
This could be because the errors made by the SVM are evenly distributed across instances or
because the errors made by the other models are large for a few instances but smaller for the rest.

% Table wit hall used machine learning models and their metrics
\begin{table}[H]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
% \sisetup{group-minimum-digits = 4}
        \centering
        \begin{tabular}{llll}
            \toprule
            \thead{\textbf{Model Name}} & \thead{\textbf{MAE}}
            & \thead{\textbf{RMSE}} & \thead{\textbf{Relative Error}} \\
            \toprule
            \textbf{Decision Tree} & 0.275 & 0.401 \\
            \hdashline
            \textbf{Linear Regression} & 0.228 & 0.283 \\
            \hdashline
            \textbf{Ada Boost} & 0.250 & 0.293 \\
            \hdashline
            \textbf{Gradient Boosted Trees} & 0.193 & 0.261 \\
            \hdashline
            \textbf{Random Forest} & 0.220 & 0.277 \\
            \hdashline
            \textbf{Extra Trees } & 0.216 & 0.261 \\
            \hdashline
            \textbf{Multi-Layered-Perceptron} & 0.161 & 0.205 \\
            \hdashline
            \textbf{Support Vector Machine} & 0.128 & 0.170 \\
            \bottomrule
        \end{tabular}
    \end{tcolorbox}
    \caption{\textbf{Results for the Design Principle 1 Correctness:} The results are sorted by RMSE in ascending
    order.}
    \label{tab:results-correctness}
\end{table}


As already noted most models have a RMSE of 0.2 which means that the error for a prediction is 0.2~mm.
For example if the spring back is 0.5 mm the model predicts could predict the spring back between 0.3~mm and 0.7~mm.
To evaluate the the models the question still remains if this is an acceptable error or not.
To answer this question the tolerance from the DIN 2768 which is 0.5\degree.
In \cref{tab:examples-tolerance} multiple sample from the dataset where chosen and their tolerance was calculated.
The examples chosen are the same as in \cref{sec:demonstration} where it was also explained why they where chosen,
the goal was to choose samples with a wide range of spring back values and different input values.

Since the spring back in the dataset was measured in millimetres but the tolerance is given in degrees the following
calculated was applied to convert the spring back to degrees:

\begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
    \begin{equation}
        \arctan\left(\frac{l/2}{y_p}\right)\cdot\frac{180}{\pi}
    \end{equation}
\end{tcolorbox}

Where $l$ is the support distance, which is the distance where the metal sheet is supported on the left and on the
right in the experimental setup.
The resuls in \cref{tab:examples-tolerance} show that the tolerances for the chosen examples are between 0.09\degree and
0.27\degree which means that all of the examples are within the tolerance.
This means that the \ac{ML} models in this study are able to predict the spring back with an accuracy that is well
within the tolerance of the highest accuracy level denoted in the DIN 2768.
That is enough for most applications used in the industry.

\begin{table}[H]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
% \sisetup{group-minimum-digits = 4}
        \centering
        \begin{tabular}{ll}
            \toprule
            \textbf{Sample}            & \textbf{tolerance} \\
            $t$: 3 $V$: 20 $y_p$: 10   & \pm 0.27\degree    \\
            $t$: 3 $V$: 20 $y_p$: 20   & \pm 0.16\degree    \\
            \hdashline
            $V$: 20 $t$: 1.5 $y_p$: 10 & \pm 0.16\degree    \\
            $V$: 20 $t$: 1.5 $y_p$: 20 & \pm 0.09\degree    \\
            \hdashline
            $V$: 50 $t$: 0.5 $y_p$: 10 & \pm 0.18\degree    \\
            $V$: 50 $t$: 0.5 $y_p$: 20 & \pm 0.10\degree    \\
            \bottomrule
        \end{tabular}
    \end{tcolorbox}
    \caption{\textbf{Results for the Design Principle 1 Correctness:} The results are sorted by RMSE in ascending
    order.}
    \label{tab:examples-tolerance}
\end{table}

The overall performance of the machine learning models are good and the best performing models are the Extra Trees and
Support Vector Machine models.


\section{DP2: Relevance}\label{sec:relevance}
% Does the model achieve a good bias-variance tradeoff? Which means neither
% overfitting or
% unterfitting the data.

% Bias variane trade-off
A model is considered relevant when it achieves a balance between bias and
variance, avoiding both overfitting and underfitting of the training data.
The relevance of the model can be quantified through the \textit{variance of cross-validation}, which proves insight
into how the model performs when trained and
evaluated on different subsets of data and how well it generalizes.

% Explanation interpretation of variance
A low variance indicates that the model's performance is consistent across
different folds, suggesting that the model is not overfitting the training data.
Conversely, a high variance implies that the performance can vary significantly
depending on the specific data points used in the test set, indicating a potential
overfitting problem.

A common metric for gauging the effectiveness of regression models is the $R^2$ statistic, it shows how much of the
the variance is explained by the model.
It ranges usually from 0 to 1, where 0 means that the model does not explain any of the variance and a score of 1
means that the model fully explains the variance
~\cite[p. 43]{muller_introductionmachinelearning_2016}.

\cref{eq:r2} shows the formula for the $R^2$ score as explained by
~\cite[p. 43]{muller_introductionmachinelearning_2016}.

\begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
    \begin{equation}
        \label{eq:r2}
        R^2 = \frac{\text{Explained variance}}{\text{Total variance targert variable}}
    \end{equation}
\end{tcolorbox}

One unique feature of scikit-learn's implementation of $R^2$ is that the score can be negative if the model performs
poorly.
For instance, a model would receive a $R^2$ score of 0.0 if it consistently predicted the expected value of y
regardless of the input features
~\cite{_sklearnmetricsr2_}.
In order to achieve a negative $R2$ score with sci-kit learns' implementation a model has to perform worse than a
model that simply predicts the mean value of the dependend variable.
Therefore, a high $R^2$ score indicates a good model fit and a good bias-variance
tradeoff~\cite[p. 43]{muller_introductionmachinelearning_2016}.

\subsection{Results}\label{subsec:results3}

\cref*{tab:results_relevance} displays the variance of cross-validation and the $R^2$ for all the machine
learning models used, sorted in ascending order of their $R^2$ score.
The variance of cross-validation was calculated using Scikit-Learn's \texttt{cross\_val\_score} method, and five
-fold cross-validation was used for the calculation.
The formula shown in \cref{eq:r2} was used to calculate the $R^2$ score.

It can be observed that most of the models have relatively low variance, indicating that their performance is
consistent across different folds.
Based on the table the following models have the best bias-variance tradeoff:

\begin{enumerate}
    \item Extra Trees: With an $R^2$ of 0.889 and a Variance of CV of 0.031, this model performs the best among
    the listed models. It has a high $R^2$ value, meaning it can explain a significant proportion of the
    variance in the data, and a relatively low Variance of CV, indicating stable performance across different
    subsets of the data.
    \item Multi-Layer Perceptron (MLP): The MLP model has the second-highest $R^2$ value of 0.825 and a low Variance
    of CV of 0.023, which implies that it can explain a substantial amount of variance in the data and perform
    consistently across different subsets.
    \item Random Forest: The Random Forest model has an $R^2$ value of 0.784 and a Variance of CV of 0.041.
    Although the model explains less variance.
\end{enumerate}


\begin{table}[h]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \centering
        \begin{tabular}{lll}
            \toprule
            \thead{\textbf{Model Name}} & \thead{\textbf{Variance of CV}}
            & \thead{\textbf{$R^2$}} \\
            \toprule
            \textbf{Support Vector Machine} & 0.016 & 0.893 \\
            \hdashline
            \textbf{Extra Trees}            & 0.031 & 0.889 \\
            \hdashline
            \textbf{Multi-Layer-Perceptron} & 0.023 & 0.825 \\
            \hdashline
            \textbf{Random Forest}          & 0.041 & 0.784 \\
            \hdashline
            \textbf{Gradient Boosted Trees} & 0.021 & 0.761 \\
            \hdashline
            \textbf{Ada Boost}              & 0.072 & 0.676 \\
            \bottomrule
        \end{tabular}
    \end{tcolorbox}
    \caption{Performance of the used machine learning models according
    relevance.}
    \label{tab:results_relevance}
\end{table}


\section{DP3: Robustness}\label{sec:robustness}
% Definition of robustness

Unlike \ac{DP} Correctness(\cref{sec:dp1:-correctness}), robustness is a non-functional characteristic of a \ac{ML}
model.
A way to evaluate the robustness is to check the correctness of the model with added noise to the
data
~\cite[p. 1]{saez_evaluatingclassifierbehavior_2016}, if the model is still able to predict the correct values, it is
considered robust.
As mentioned in \cref{subsec:dataset-exploration} the data set was generated in a controlled environment does not
contain many data quality issues and noise as they where removed during the process.
This offers the opportunity to test the robustness of the models by manipulating the data set to introduce the data
quality issues missing values and noise.
To do that two tests where developed and are described in the following sections.

\subsection{Missing Data}\label{subsec:missing-data}
When applying the model to real-world data, it is possible that some values are missing.
Looking at the available dataset two scenarios or missing data can occur:
Missing $Vt$-pairings and missing values.

In the first scenario, there may be no data available for a specific die opening, which can be due to the die opening
was never being used before or a lack of recorded information.
In the second scenario, while data may exist for the desired $Vt$ combination, certain values may
be missing or the dataset may not be complete.

To address these scenarios, two tests where conducted to evaluate the robustness of the models.
The first test shows how the model behaves when there is no data available for a specific die opening.
The second test shows how the model behaves when noise is added to the training data.
Both test will be described in
the following sections.

\subsubsection{Missing values}

Initially, the use of the \ac{LPOCV} method was considered.
However, this approach was omitted due to computational constraints, as it generates all possible
training and test sets by removing $p$ samples from the complete dataset, resulting in a large
number of overlapping test sets and a high computational cost.
As an alternative, regular CV was applied, with an increasing number of folds.
The minimum number of folds was two and the maximum was equal to the total number of samples in the
dataset.

Unfortunately, the experiment did not yield distinguishable results, indicating that all models performed similarly.
As a result, no conclusions could be drawn.
While various metrics were tested for the CV score, none provided any useful insights.
It is unclear why this particular test did not produce results, but it may have been due to the specific combination
of dataset and model used in this study.
Nevertheless, this approach remains a viable option for future research and should be considered.

The third and final experiment utilized random sampling to create different train-test data compositions, allowing
for the evaluation of the model's handling of missing data.
Split ratios from 90:10 to 10:90 were used, with a step size of 10.
This approach was inspired by a similar method used in a previous study~\cite[p. 570--574]{liu2021deep}  which used
thee different split ratios to evaluate the performance of a deep learning model.

The results of this experiment provided valuable insights into the model's performance under
different data compositions, particularly when dealing with missing
data, and were used in conjunction with other metrics to gain a comprehensive understanding of
the model's overall performance.

\cref{fig:results-missing-values} shows how the models performed when trained on
continuously less data.
It can be seen that all models perform well when trained with on 90\% of the data.
However, when trained on 50\% of the data, the \ac{MLP}  and \ac{GBT} models start to perform
better than the other models.
When trained on 20\% of the data and less all models significantly underperform.

The results show that having a small dataset doesn't necessarily mean that the results will be bead.
In fact, the results indicate that even with only 400 samples in the dataset, half of that would sitll produce good
results.
This implies that data quality plays a crucial role in determining the success of a model rather than the quantity of
data.
However, it is important to note that a dataset's size should not be too small, as that would be less than 200
samples in this case, which could lead to unreliable results.
The results who that even half of the only 400 sample string dataset would still yield good results.

\begin{figure}[h]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \centering
        \includegraphics[width=0.6\textwidth]{chap5/images/missing_values_plot}
    \end{tcolorbox}
    \caption{Comparison of performance on less training data.}
    \label{fig:results-missing-values}
\end{figure}

Looking at the variances shown in \cref{fig:variance-missing-values} the the \ac{SVM} and \ac{MLP} models
perform the best and seem to be the most robust models when trained on less data.
It has to be noted, that as seen in the previous figure the most variance comes from training on
below 20\% of the data.

Overall the \ac{MLP} model seems to be the most robust model, as it performs well on all data
compositions and has the lowest variance.

\begin{figure}[h]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \centering
        \includegraphics[width=0.6\textwidth]{chap5/images/variance_missing_values}
    \end{tcolorbox}
    \caption{Variance of models when trained on different data.}
    \label{fig:variance-missing-values}
\end{figure}

\subsection{Noise}\label{subsec:noise}
A common practise in \ac{ML} is to add noise to the training data to improve
the model's generalization abilities.
In this study Gaussian Noise will be added to the data set to evaluate the robustness
of the model.
The noise will be added to the training data only as the training data should be as correct as possible to get
comparable results.
To add the noise, \textit{numpy.random.normal} function was used, which generates random numbers with a Gaussian
distribution
~\cite{harris2020array}.
The mean and standard deviation of each feature were calculated based on the original data, and the function was
used to generate noisy values for each feature.

To study the model's reaction to increasing amounts of noise, the noise added
to the training data was gradually increased starting from 1\% to 50\%, which means that in the
last iteration contains as many noisy as `clean' samples.
The difference between all \ac{RMSE} between iterations was defined as loss.
The total loss was calculated by averaging the difference of these values as shown in Equation~\ref{eq:noise}.

\begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
    \begin{equation}
        \text{Average Loss} = \frac{1}{N} \sum_{i=1}^{N} |\text{RMSE}i -
        \text{RMSE}{avg}|\label
        {eq:noise}
    \end{equation}
\end{tcolorbox}

\begin{itemize}
    \item \textit{Does the calculation of the loss make sense?}
\end{itemize}

\subsection{Results}\label{subsec:results-robustness}
\cref{fig:results-noise-fig} presents the performance of models as more noise is introduced to the training
dataset.
The noisy dataset comprised 1000 samples, and the x-axis indicates the percentage of the original data that
was augmented with noise.
The dashed lines correspond to the default performance of the models in the absence of noise.
The results indicate that the performance of all models deteriorates rapidly with the addition of noise.
Specifically, the first 20\% of the noisy data causes the most significant decline in model performance, after which the
performance plateaus at a poor level.
It is noteworthy that the Extra Trees model exhibits the most robust performance, outperforming the Random Forest
and Gradient Boosted Trees models, which also demonstrate similar levels of resilience.

The results are interesting since they demonstrate that the typically high-performing SVM and MLP models
exhibit underperformance under the current conditions.
Specifically, the MLP is a highly expressive model with many parameters, making it prone to overfitting.
In this case, the addition of noise is likely causing the model to overfit to the noise, leading to poor
generalization performance.
On the other hand, the SVM aims to find an optimal hyperplane to separate the data with maximum margin, which makes
it highly sensitive to outliers and noise, making it challenging to identify the appropriate hyperplane.

These results emphasize the significance of selecting robust models and pre-processing the data to minimize the
presence of outliers.
Moreover, the study reveals that even a small amount of noise (i.e., 10\%) can significantly impact the model's
performance.
The significance of using a high-quality dataset is emphasized.
Also, the results indicate that a smaller dataset can still be effective in model training as long as the data
quality is good.

Also this test underlines the previous results, that relatively little data is enough to get good results as already
little noise is enough to get bad results.
This means that a good data quality outweighs a small dataset, as long as the dataset is not too small which would
be below 200 samples in this case.


\begin{figure}[h]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \centering
        \includegraphics[width=0.9\textwidth]{chap5/images/results_noise}
    \end{tcolorbox}
    \caption{Comparison of performance on noisy data.
    The x-axis shows how many percent of the noisy dataset where added to the training data.
    The y-axis shows the RMSE metric.
    The dotted lines show the default performance of the model.
    The solid lines show the performance of the model when noise was added to the training data.
    }
    \label{fig:results-noise-fig}
\end{figure}


\section{DP4: Stability}\label{sec:stability}
To evaluate the stability of the models, the Leave-One-Out Cross-Validation (LOOCV) can be used.
LOOCV, a type of Cross-Validation in which one sample is used for validation and the remaining data is used for
training, is one appropriate way to measure a model's stability.
~\cite[p. 200--201]{gareth2013introduction}.
To evaluate the model's stability, the \ac{LOOCV} was repeated for all samples in the dataset, resulting in a total
of $n$ iterations.
The stability of the model was then determined by calculating the average prediction error across all iterations,
using the equation provided in \cref{eq:loocv}~\cite[p. 201]{gareth2013introduction}.

\begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
    \begin{equation}
        CV_{(n)} = \frac{1}{n} \sum_{i=1}^{n} \text{MSE}_{i}\label{eq:loocv}
    \end{equation}
\end{tcolorbox}

Usually the MSE is used as the error metric since it is not needed the RMSE is just the square root of the MSE and
therefore does not provide any additional information.
Additionally the resulting (\ac{MSE}) is a poor estimate of the model's
generalization error since only one sample is used for validation.
But the metric can be used to evaluate the model's stability, which is the goal of this section.
The final stability is determined by the standard deviation of the cross-validation scores
~\cite[p. 201]{gareth2013introduction}.
A low standard deviation across all folds indicates that the model can generate consistent results when trained on
different datasets, which suggests a more stable model.

It is worth mentioning that there are numerous methods for determining a model's stability.
In this study, \ac{LOOCV} was chosen for several reasons:
First, \ac{LOOCV} uses almost all the available data for training, while k-fold cross -validation reserves a portion
of the data for testing.
Since the dataset used in this study is relatively small, using all the data for training is advantageous.
Second, \ac{LOOCV} typically has lower bias due to the larger training set.
Lastly, various approaches were attempted to measure the model's stability, and \ac{LOOCV} delivered
the most stable and interpretable results.
The primary drawback of this method is its computational expense since it
requires $n$ iterations.

\cref{subsec:results-stability} presents the cross-validation score and standard deviation for each model.
The focus of the analysis is on the standard deviation since it provides a better indication of the model's stability.
As mentioned earlier, the \ac{CV} score using \ac{LOOCV} is a poor estimator of the model's generalization error.

\subsection{Results}\label{subsec:results-stability}

\cref{tab:results-stability} shows the mean cross-validation score and standard deviation for each model.
In order to evaluate the stability of the model, the standard deviation was used as the primary metric.
The stability analysis using LOOCV reveals that the Gradient Boosting model is the most stable with a standard
deviation of 0.183.
The Etra Trees and Random Forest model exhibit similar levels of stability with mean CV scores of 0.047 and 0.049,
and standard deviations of 0.206 and 0.194
It can be observed again, similar to the evaluation of the robustness that the MLP and SVM models exhibit
lower stability levels compared to the other models even though they where the most promising in the evaluation of
the correctness.

While stability is an important factor to consider, other aspects in particular the generalization error (correctness)
should be also taken into account.

\begin{table}[h]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
% \sisetup{group-minimum-digits = 4}
        \centering
        \begin{tabular}{ll}
            \toprule
            \thead{\textbf{Model Name}} & \thead{\textbf{mean cv score ± std}}
            \\
            \toprule
            \textbf{Gradient Boosting}      & 0.038 ± 0.183 \\
            \hdashline
            \textbf{Random Forest}          & 0.047 ± 0.206 \\
            \hdashline
            \textbf{Extra Trees}            & 0.049 ± 0.194 \\
            \hdashline
            \textbf{Support Vector Machine} & 0.053 ± 0.244 \\
            \hdashline
            \textbf{MLP}                    & 0.053 ± 0.244 \\
            \hdashline
            \bottomrule
        \end{tabular}
    \end{tcolorbox}
    \caption{Results of the stability of the models.}
    \label{tab:results-stability}
\end{table}


\section{DP5: Resource utilization}\label{sec:resource-utilization}

To evaluate the resource utilization of the model, several metrics can be employed.
As discussed in \cref{subsec:dp5-resource-utilization}, this study focuses on time, computational power,
and memory resources.
Consequently, the metrics utilized to assess resource utilization include training time, inference time, and memory
usage.

The training time is measured in seconds and refers to the time that is required to train the model.
Training a model need computational resources such as memory and CPU power, thus the longer the training process
takes the more resources are needed.
Therefore a shorter training time is preferred.
To measure the training time the python function \texttt{time.time} is employed.
The functions does return the time in seconds since the epoch.
The time is recorded when the model is fitted and the difference between the two values
represents the training time.

The inference time is measured in milliseconds and refers to the time the models need to make predictions.
This metric is important for real-world use cases because user do not want to wait long for predictions and the
process also needs computational resources.
Since most models are able to make one predictions quite fast the inference time is measured for 100 predictions
using the \texttt{time.time} function again.

The memory usage is measured in megabytes (MB) and refers to the amount of memory that is required to run and save
the mode.
A smaller memory footprint is preferred because it required less resources in the form of storage space.
To measure the memory usage the \texttt{memory\_usage} function from the package \texttt{memory\_profiler} is
employed.


\subsection*{Results}

\cref{tab:results_resource_utilization} shows the results for the text that where made.
Looking at the training time the Extra Trees and Random Forest model perform the with the shortest training times
of 19.541 and 24.912 ms
The Support Vector Machine and Gradient boosted model have intermediate training times.
The MLP is clearly the slowest with over 16 seconds.
The training time can be attributed the nature of the models and their training algorithm.
The RF and ET both benefit from the bagging technique which means that they training an ensemble of decision trees and
combine them to one big model.
This can be done in parallel and therefore massively speeds up the training
process (see \ref{subsubsec:random-forests}).
On the other side MLPs are typically more computational since feeding forward and backpropagation are not
parallelize and computationally expensive in general (see \cref{subsec:neural-networks}).

A comparable picture can be derived for the inference time.
This time the GBR, which is also a tree based algorithm is the fasted to make predictions while the MLP is the slowest.
The ET, RF and SVM models have comparable times.

Looking at the memory usage the models are quite similar with the MLP using the most memory 170 KB.
The other models have quite similar memory usage ranging from 157 to 158 KB.
Overall the memory space seems not to be a big issue for the models, a reason for that is the fact that the
dataset is not very big and the resulting models are also less complex.


\begin{table}[h]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \centering
        \begin{tabular}{llll}
            \toprule
            \thead{\textbf{Model Name}} & {\thead{\textbf{Training time} \\
            \unit[]{ms}}}
            & {\thead{\textbf{Inference time} \\ \unit[]{ms}}} &
                {\thead{\textbf{Memory Usage} \\
            \unit{kb}}}
            \\
            \toprule
            \textbf{ET}  & 19541    & 1.939  & 157.93 \\
            \hdashline
            \textbf{RF} & 24912    & 2.027  & 157.684 \\
            \hdashline
            \textbf{GBR} & 43688   & 1.302  & 158.121 \\
            \hdashline
            \textbf{SVM} & 421007 & 2.587 & 158.082 \\
            \hdashline
            \textbf{MLP} & 16466261 & 17.988 & 170.715 \\
            \bottomrule
        \end{tabular}
    \end{tcolorbox}
    \caption{Overview of the used machine learning models and their metrics.}
    \label{tab:results_resource_utilization}
\end{table}


\section{DP6: Interpretability}\label{sec:interpretability}
In accordance with the definitions of interpretability outlined in \cref{sec:objectives-of-a-solution}, it
becomes apparent that quantitatively measuring interpretability is not viable.
However, alternative approaches can be employed to gauge a model's interpretability.

%\cite{molnar2020interpretable} distinguishes between model-specific and model-agnostic approaches.
%Model-specific procedures are particular to one model, but model-agnostic methods can be applied to any model.
%~\cite[p. 19]{molnar2020interpretable}.

One strategy for achieving interpretability is restricting the selection of algorithms to those that yield
model specific interpretable outcomes.
Interpretable algorithms used in this study are Linear Regression and Decision
Trees~\cite[p. 35]{molnar2020interpretable}.
Nonetheless, as discussed in \cref{sec:dp1:-correctness}, these algorithms do not demonstrate sufficient
performance to be deemed viable solutions for predicting spring back.
Consequently, this study cannot rely on model-specific interpretability methods and should instead utilize model
-agnostic methods for generating explanations.

Also~\cite{doshi2017towards} differentiates between global and local interpretability.
Global interpretability refers to the ability to understand the overall behavior of
the model, while local interpretability refers to the ability to understand the behavior of the
model for a specific instance~\cite[p. 7]{doshi2017towards}.
\cite{doshi2017towards} note that global interpretability is important when scientific understanding or the detection
of bias is the goal while local interpretability is important when the goal is to understand a specific prediction of
the trained model
~\cite[p. 7]{doshi2017towards}.

It does not make sense to apply model-agnostic method on all trained models, it makes more sense to
apply the methods on the best performing models.
% TODO and Extra Trees...
In the other parts of the evaluation, the Random Forest, Support Vector Machine and Multi Layer Perceptron models
where always under the best perfoming models and are therefore the models that are used in the following
sections.

\subsection{Global Model-Agnostic Methods}\label{subsec:global-model-agnostic-methods}
In the following section, global model-agnostic methods are applied to the trained machine learning models to
understand their overall behavior.

% TODO Source?
Two commonly used methods for analyzing the relationship between input features
and the target variable are Feature Dependence Plots (FDP) and Partial Dependence Plots (PDP).
Feature dependence plots illustrate the relationship between a feature and the target variable.
By analyzing these plots, we can identify which features have the strongest impact on the model's
predictions, making it easier to prioritize and refine our feature selection.

In the case of \ac{MLP}, there is no built-in feature importance, as the importance of each feature is determined by
the weights assigned to the perceptron.
For \ac{SVM}, feature importance cannot be directly derived from the model
because the algorithm applies a hyperplane that separates the data.
The importance of a feature depends on the influence of the hyperplane, which is not easily measurable.
Consequently, feature importance cannot be plotted for
the two chosen models.
In contrast, other models, such as random forests, have built-in feature importance that can
be utilized to analyze the model.


\ref{fig:feature-importances-rf1} compares and visualizes the relative importance of the features used for
training the model.
The figure was created using the \texttt{feature\_importances\_} attribute of the \texttt{RandomForestRegressor}
model from the \texttt{scikit-learn} library.
The Partial Dependence Plots (PDP) provide insights into the relevance and interactions of the
features.
As illustrated, thickness is the most important feature, followed by distance and die opening.
This means that changes in thickness have the most significant impact on spring back, while variations in the other
two features
have a comparatively smaller influence on the outcome.
In terms of interactions, all features are relevant, indicating that each feature contributes unique information to
the model.
This could suggest that the features are not highly correlated with each other, as their interactions
could already be observed in the correlation matrix shown in Figure~\ref{fig:correlation_matrix}.
Consequently, removing one feature would result in a significant loss of information and, therefore, poorer
performance of the model.

\begin{figure}[h]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \centering
        \includegraphics[width=0.7\textwidth]{chap5/images/rf_feature_importances}
    \end{tcolorbox}
    \caption{Feature importance of the random forest model.}
    \label{fig:feature-importances-rf1}
\end{figure}

The feature importance plot did facilitate a better understanding of the relationship between the features and the
target
variable.
However, they do not provide insights into exactly how the features influence the outcome.
Partial dependence plots can be helpful for this purpose.

\subsubsection{Partial Dependence}

Another tool to measure the importance measure is Partial Dependence Plots (PDP).
They measure the relationship between a target variable and a single predictor variable in a statistical model, while
keeping all other predictor variables constant
~\cite[pp. 313--314]{boehmke2019hands}.
A feature that exhibits a consistent partial dependence across all values is less important than a feature that
exhibits a non-linear relationship with the target variable.
While the feature importance plot shows the relative importance of each feature, the partial dependence plot shows
the impact of a single feature on the target variable.
Sci-kit learns's implementation of PDP plots is widely used and also utilized in this study
~\cite{scikit-learn}.


In \cref{fig:partial_dependence_plots}, we can see the PDPs for two models: Support Vector Machines and
Multilayer Perceptron.
To the best knowledge of the author this is the first time that PDPs are used to analyze the relationship between
the input features and the target variable in the context of spring back prediction.

It can be seen that the plots for both models yield the same results for the three features.
The results show that a higher $y_p$ value leads to a greater spring back, while
increasing the thickness of the metal sheet results in a lower spring back.
The effect of the die opening is less significant than the other two features, but there is still a noticeable
increase in spring back with higher die opening.

The $y_p$ is the main factor that influences the bending angle and since most applications require a specific angle
this is not a viable option.
The results show that the most viable option to reduce the spring back in practical applications is to reduce the
thickness of the metal sheet.
As shown in \cref{fig:partial_dependence_plots}, especially for small thicknesses a change of 0.5 mm can lead to a
significant reduction of the spring back.
Since the same bending angle usually chan be achieved with multiple die openings it is also recommended to use a
smaller die opening to reduce the spring back as well.
The PDPs demonstrate that all features exhibit a non-linear relationship with the target variable.
This information can be useful for understanding the behavior of the models and the impact of different features on
the predicted outcome.

\begin{figure}[h]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \centering
        \begin{subfigure}{0.45\textwidth}
            \includegraphics[width=\textwidth]{chap5/images/partial_dependence_SVM}
            \caption{SVM}
            \label{fig:feature_impoartances_rf}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.45\textwidth}
            \includegraphics[width=\textwidth]{chap5/images/partial_dependence_MLP}
            \caption{MLP}
            \label{fig:partial_dependence_svm}
        \end{subfigure}
        \hfill
    \end{tcolorbox}
    \caption{Partial dependence plots generated with \textit{sci-kit learn}~\cite{scikit-learn}}
    \label{fig:partial_dependence_plots}
\end{figure}

It is important to note that partial dependence plots and feature importance plots are limited to
identifying linear relationships between features and the target variable.
Since this is not the case as state previously, the results of the partial dependence plots are
alternative models as permutation feature importance values are used to gain further
information.

\subsection{Permutation Feature Importance}\label{subsec:permutation-feature-importance-
and-shap}

Permutation feature importance's assesses the impact on the model's prediction error when the values
of a feature a randomly permutes.
Permutation in this context means the process of randomly shuffling the values of a single features while keeping the
values of all other features unchanged.
Thereby the association between the feature and the actual outcome is disrupted~\cite{_permutationfeatureimportance_}
%~\cite[p. 157]{molnar2020interpretable}.
The importance of a feature is evaluated by assessing the degree to which permuting its values
results in an increase in the prediction error fo the model.
If permuting a features results in a significant increases in errors the features is deemed
important because the model relied on it for making accurate predictions.
Conversely, if permuting a feature doesn't change the prediction error is considered unimportant
because the model didn't utilize it for its prediction
~\cite{_permutationfeatureimportance_}.
%~\cite[p. 158]{molnar2020interpretable}.

The results for the MLP and SVM models presented in \cref{tab:permutation_feature_importance}.
The top section of the table shows the permutation importance of the feature for the $R^2$ while the bottom section
show the permutation importance for the $MSE$.
In this case again the $MSE$ was chosen since the $RMSE$ would not provide any additional information.
For each feature the permutation feature importance score as well as the standard derivation is listed.
If a feature achieves a higher permutation importance score it is considered to be more important, since it does
influence the outcome of the model more.
Additionally the standard derivation is given, a higher standard derivations suggest that the feature importance
score is more unstable.
The results show that the MLP and the SVM models yield similar trends in the importance of features.
The features thickness and punch penetration have a high importance with their permutation causing a significant
increase in the prediction error both in $R^2$ and $MSE$.
This indicates that they are crucial for the decision making of the model.

What differs from the previous analysis: The die opening has a importance of zero in both models.
Ths implies does permuting this features does not have any influence on the prediction error.
This is a surprising result and there are two possible explanations for this:
\begin{enumerate}
    \item The die opening is not relevant for the decision making at all and does not provide any
    significant information.
    \item Or on the other hand the die opening is important but only in combination with either the thickness or the
    penetration distance.
    This suggest that there might be an interaction effect that was not captured in previous analysis.
\end{enumerate}

Since the normal feature importance plots showed that die opening is important the second explanation is more
likely.
To further investigate this possibility a partial dependence plot was created specifically for the die opening in
combination with either the thickness or the penetration distance.

\begin{table}[h]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \centering
        \subfloat[Subtable 1 list of tables text][MLP]{
            \begin{tabular}{lll}
                \toprule
                \multicolumn{3}{}{\textbf{$R^2$}} \\
                \toprule
                thickness   & 1.337 & +/- 0.173542 \\
                \hdashline
                distance    & 1.293 & +/- 0.145488 \\
                \hdashline
                die opening & 0.000 & +/- 0.000000 \\
                \toprule
                \multicolumn{3}{l}{\textbf{$MSE$}} \\
                \toprule
                thickness   & 0.274 & +/- 0.035613 \\
                \hdashline
                distance    & 0.265 & +/- 0.029856 \\
                \hdashline
                die opening & 0.000 & +/- 0.000000 \\
                \bottomline
            \end{tabular}}
        \qquad
        \subfloat[Subtable 2 list of tables text][SVM]{
            \begin{tabular}{lll}
                \toprule
                \multicolumn{3}{}{\textbf{$R^2$}} \\
                \toprule
                distance    & 1.237 & +/- 0.139285 \\
                \hdashline
                thickness   & 1.178 & +/- 0.157247 \\
                \hdashline
                die opening & 0.000 & +/- 0.000000 \\
                \toprule
                \multicolumn{3}{l}{\textbf{$MSE$}} \\
                \toprule
                distance    & 0.254 & +/- 0.028583 \\
                \hdashline
                thickness   & 0.242 & +/- 0.032269 \\
                \hdashline
                die opening & 0.000 & +/- 0.000000 \\
                \bottomline
            \end{tabular}}
    \end{tcolorbox}
    \caption{Permutation feature importance}
    \label{tab:permutation_feature_importance}
\end{table}

To investigate the joint performance of the die opening feature with the other two features,
again partial dependence plots can be used.
\cref{fig:dpd-distance-die-opening} shows a two-way partial dependence plot for the
\ac{MLP} model.
It is segmented into three individual plots: The first two plots show the partial dependen of the distance and the
die opening.
But the most interesting is the third plot which the interaction of the two input features distance (x-axis) and die
opening (y-axis).
The slope of the contour lines indicates the direction of the effect of the die opening on the
prediction.
The contour lines are colored and labeled according to the value of the prediction.
It can be seen that the contour lines are not parallel which suggest the two features are
interaction with each other and therefore have a joint effect on the model prediction.

This means that the second explanation mentioned above is correct and the die opening is not irrelevant for the model
prediction but just interacts with the other two features.
Therefore it only affect the prediction in combination with the other two features.

\begin{figure}[h]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \centering
        \includegraphics[width=0.8\textwidth]{chap5/images/pdp_distance_die_opening}
    \end{tcolorbox}
    \caption{DPD distance and die opening}
    \label{fig:dpd-distance-die-opening}
\end{figure}

\subsection{Local Model Agnostic Methods}\label{subsec:local-model-agnostic-methods}
Local interpretability refers to the ability to understand the behavior of the model for a
specific instance and is often used to explain the predictions of a model
~\cite[p. 19]{hall2019introduction}.
Model agnostic methods can be used to explain the predictions of any model type and not only specific algorithms
~\cite[p. 20]{hall2019introduction}.

Local methods rely on interpreting individual instances from a dataset, and as such, it is
crucial to select representative samples.
One way to achieve this is by considering the V/t ratio of each instance.
The local model agnostic method LIME was applied on different machine learning models.
But it did not yield any meaningful results.


\section{Demonstration}\label{sec:demonstration}
This section represents the demonstration activity in the \ac{DSR} process.
This activity focuses on the demonstration of the developed models in specific selected scenarios and not the
overall performance like in the evaluation activity.
For this purpose three scenarios are selected and the models are demonstrated in these scenarios.

For the current dataset, the V/t ratios range from 10 to 100.
The highest ratio can be achieved by bending a 0.5 mm thin metal sheet using a 50 mm wide die opening, while the
lowest ratio is achieved by bending a 1 mm thick sheet on a 10 mm die opening.

Recommended V/t ratios in industrial practices are between 6 and 10~\cite[p.7]{cruz_applicationmachinelearning_2021}.
Bending operations performed outside of this range of recommended ratios may result in
high spring back.
The lowest V/t ration in the dataset is 10, after that it was not possible to squeeze metals larger than 1 mm
through a die opening of 10 mm.
This means that the ratios used in industrial practise do not exactly apply of the dataset used in this study.

Upon examining \cref{fig:springback-heatmap}, it is evident that the V/t ratio has a significant impact on
spring back.
There appears to be a connection between a greater die opening and a reduced thickness resulting in greater spring
backs. This was also observed in \cref{subsec:dataset-exploration}.
Notably, the data points for $t0.5 V40$ and $t0.5 V50$ exhibit particularly elevated spring backs relative to the
remaining data set.
Conversely, all the data collected with a 10 mm die opening indicate lower spring back values.

\begin{figure}[h]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \centering
        \includegraphics[width=0.8\textwidth]{chap5/images/mean_springback_heatmap}
    \end{tcolorbox}
    \caption{Mean spring back for a V/t combinations. The x-axis denoted the thickness WHILE THE y-axis denotes the
    die opening}
    \label{fig:springback-heatmap}
\end{figure}

To select specific instances for the demonstration, the V/t ratios of the instances are considered.
A V/t ratios around 20 is considered to be in the middle of the the dataset.
Therefore three cases are defined, where the V/t ratios are below, within and above the recommended range.

The selected representative instances for the local methods are from one of three cases:
\begin{itemize}
    \item Case A includes instances with V/t ratios below the recommended range.
    \item Case B includes instances within the recommended range of V/t ratios.
    \item Case C includes instances with V/t ratios above the recommended range.
\end{itemize}

The chosen instances from each case are shown in \cref{tab:representative-instances}.
The table shows the instances chosen grouped by the fact if there in, below or above the recommended range.

\begin{table}[h]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \centering
        \begin{tabular}{lllll}
            \toprule
            \textbf{Case} & \textbf{\(V\) } & \textbf{\(t\)} & \textbf{V/t} & \textbf{In Range} \\
            \toprule
            A             & 20              & 3.0            & 6.66         & below             \\
            \hdashline
            B             & 30              & 1.5            & 20           & in                \\
%            B             & 20              & 1.5            & 13.33        & in                \\
            A             & 20              & 2.0            & 10           & in                \\
            \hdashline
            C             & 40              & 0.5            & 80           & above             \\
            C             & 50              & 0.5            & 100          & above             \\
            \bottomrule
        \end{tabular}
    \end{tcolorbox}
    \caption{Representative instances for the local methods.
    V denots the die opening, t denotes the thickness of the metal sheet and V/t denotes the V/t ratio.
    The last column indicates if the V/t ratio is in, below or above the recommended range.}
    \label{tab:representative-instances}
\end{table}

\subsection{Comparison of the Models}\label{subsec:overall-comparison-model-performance}
In this section, the outcomes generated by the trained models on the selected cases A, B, and C are presented.
To ensure unbiased results, all samples that were part of the test cases were removed from the training data set.
For instance, for case A (a), all samples with a die opening of 20 and a thickness of 2 were removed from the dataset.
This was done to prevent the models from overfitting to the test cases.
It is expected that the models' performance will decline a bit in these test cases due to the removal of some
training data.

It is vital to remember that the goal values represent the mean of all values when evaluating the numbers below.
While the dataset's quality is not perfect, there may still some outliers present.
In some circumstances, there are just a few datapoints for a particular instance, therefore the mean is not a reliable
reflection of the true value.

\cref{fig:performance-case-a} depicts two chosen test cases for case A with a V/t-ratio below the recommended
range.
Test case (a) performs quite well overall and all models can predict the spring back with a mean absolute error of
less than 0.5 mm.
The weak point of the model seems to be the predictions of the edge punch penetration values and it can be seen that
the predictions is always lower than the actual value.
However, the models' performance deteriorates in instance (b) shown in \cref{fig:performance-20-3.0} with a V/t
-ratio of 6.6, particularly in the prediction of spring backs.

\begin{figure}[h]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        % Todo V20 t.2.5
%        \begin{subfigure}{0.5\textwidth}
%            \includegraphics[width=\textwidth]{chap5/images/performance_20_2.0}
%            \caption{V: 20, t: 2}
%            \label{fig:performance-20-2.0}
%        \end{subfigure}
        \hfill
        \begin{subfigure}{0.5\textwidth}
            \includegraphics[width=\textwidth]{chap5/images/performance_20_3.0}
            \caption{V: 20, t: 3}
            \label{fig:performance-20-3.0}
        \end{subfigure}
    \end{tcolorbox}
    \caption{Performance plots for case A}
    \label{fig:performance-case-a}
\end{figure}

\cref{fig:performance-case-b} illustrates two selected test series for case B, where the V/t ratios fall within
the recommended range.
These test series produced the best results compared to cases A and C.
In instances (a) and (b), all models performed reasonably well, with instance (b) displaying better overall
performance.
It is apparent that the SVM model appears to predict spring back better in case (a), while the MLP model performs
best in case (b).
Overall, it is difficult to identify a clear favorite model based on the performance of the models in these test
series.
Although some models performed better than others for specific variables or scenarios, the differences were
not significant enough to identify a clear winner.
Therefore, the selection of an appropriate model will be determined by the outcomes of the other two cases, A and C.

\begin{figure}[h]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \begin{subfigure}{0.5\textwidth}
            \includegraphics[width=\textwidth]{chap5/images/performance_20_1.5}
            \caption{V: 20, t: 1.5}
            \label{fig:performance-20-1.5}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.5\textwidth}
            \includegraphics[width=\textwidth]{chap5/images/performance_20_2.0}
            \caption{V: 20, t: 2}
            \label{fig:performance-20_2.0}
        \end{subfigure}
    \end{tcolorbox}
    \caption{Performance plots for case B}
    \label{fig:performance-case-b}
\end{figure}

Case C is the most challenging case, as all models exhibit visibly poor performance.
This is expected since the V/t ratio is well above the recommended range.
The results are unusable, as the models are unable to predict the spring back.
However, in \cref{fig:performance-50_1}, the results appear more promising, with the RF and SVM models
demonstrating the most consistent performance.
More work is clearly needed to construct machine learning models that perform well in these settings.

\begin{figure}[h]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \begin{subfigure}{0.5\textwidth}
            \includegraphics[width=\textwidth]{chap5/images/performance_40_0.5}
            \caption{V: 50, t: 0.5}
            \label{fig:performance-50_0.5}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.5\textwidth}
            \includegraphics[width=\textwidth]{chap5/images/performance_50_0.5}
            \caption{V: 50, t: 1}
            \label{fig:performance-50_1}
        \end{subfigure}
    \end{tcolorbox}
    \caption{Performance plots for case C}
    \label{fig:performance-case-c}
\end{figure}

The best overall performing models are the \ac{MLP} and the \ac{SVM} model.
As seen before, the model performs consistent across all three scenarios, indicating that it can
effectively handle a broad range of V/t ratios also outside the recommended industry guidelines.
Overall it can be said that the models are able to predict the spring back with high accuracy.
For bad predictions the error is less than 0.25 mm, which is within the tolerance of the
most manufacturing processes.


\section{Results and Discussion}\label{sec:results-and-discussion}
This component of the research is structured as follows:
The first section, titled ``Summary of Results'' presents an review of the study's findings.
The second section, ``Interpretation of Results'' delves deeper into the findings and seeks to explain them.
Both parts are structured according to the three research questions formulated
in~\cref{sec:problem-identification-and-motivation}.

\subsection{Summary of Results}\label{subsec:summary-of-results}
%Introduction
The research objective of this study is to determine whether \ac{ML} models can accurately predict the spring back of
bent sheet metal parts.
Furthermore, the study aims to identify which model or ensemble of models performs the best.
Lastly, it seeks to utilize interpretable models to gain insights into the bending process.

%Data Collection and Preprocessing

This study aims to determine the accuracy of \ac{ML} models in predicting the spring back of bent sheet metal parts,
identify the best-performing model or ensemble of models, and gain insights into the bending process using
interpretable models.

The data was collected by using the air-bending process and measuring the spring back with the output data of the
press brake.
The dataset contained 400 samples, with punch penetration, die opening, and metal sheet thickness as
input features and spring back as the target feature.
The data quality was deemed good, requiring little data preprocessing.
With a complete dataset, no data imputation was necessary.
The data was normalized using different scalers from the sci-kit learn library.

%Key Findings

\subsubsection{Hypothesis 1}

The first research question is whether ML models can accurately predict the spring back of bent sheet metal parts.
Design Principle 1 (correctness) was used to test this hypothesis using the regression metric RMSE as the main
evaluation criterion.
The results showed that most ML models were able to predict the spring back with a RMSE of less
than 0.2 mm, indicating acceptable accuracy.
The Extra Trees and Support Vector Machine models performed best with a RMSE of 0.15 mm.
The AdaBoost, Random Forest, and Gradient Boosting models demonstrated moderate prediction
performance, while the performance of Linear Regression and Decision Tree models showed poor performance and
therefore were not considered for further analysis.

\subsubsection{Hypothesis 2}

\textbf{Hypothesis 2:} Specific ML models or combinations of models (e.g., ensemble methods) will yield better
performance
in terms of the six Design Principles (DPs) compared to other models.

The second research question examines whether specific ML models or ensemble models outperform other models.
Design Principles 2 to 5 were used to test this hypothesis.

\textbf{Relevance}

Relevance was evaluated by assessing the bias-variance tradeoff of all trained models, except Linear Regression and
Decision Tree, which were ruled out.
The results showed that Extra Trees, Multi-Layer Perceptron, and Random Forest models had the best balance between
bias and variance, and therefore were considered the most relevant.
Most models were able to explain the variance in the data.

\textbf{Robustness}

The robustness of various ML models is evaluated by examining their performance in the presence of
missing data and noise.

The experiments with missing data indicate that all models perform well when trained with 90\% of the data.
However,when trained with 50\% or less, the MLP and GBT models outperform others.
When trained on less data, the SVM and MLP models perform best and appear to be the most robust.

The models are also tested with added Gaussian noise to the training data.
The performance of all models deteriorates rapidly as noise increases, with the first 20\% of noisy data causing the
most significant decline.
The Extra Trees model exhibits the most robust performance, while the Gradient Boosted Trees and Random Forest models
also demonstrate similar resilience.
Interestingly, the high-performing SVM and MLP models underperform under these conditions, likely due to overfitting
or sensitivity to outliers.

The findings emphasize the importance of using robust models and preprocessing the data to eliminate noise and
outliers.
Additionally, it was found that already more than 10\% of noise can have a significant impact on the
performance of the models.


\textbf{Stability}

Stability was evaluated using Leave One Out Cross Validation (LOOCV).
The primary metric of the evaluation was the standard deviation of the cross-validation scores.
Here the results showed that the Gradient Boosting model was the most stable with a standard deviation of 0.183,
followed by the Extra Trees and Random Forest models.

Interestingly, the SVM and MLP models, which showed good results in other Design Principles, were less stable
compared to the other models in this case.
While stability is an important factor to consider, other aspects, particularly the generalization error (correctness
), should also be taken into account.

\textbf{Resource Utilization}
The resource utilization was evaluated using three key metric: Training time, prediction time and memory usage.
This metrics should be used to evaluated the how much computational resources the trained \ac{ML} models need.

The results indicate that the Extra Tree model and the Random Forest of 19.541 ms and 24.912 ms models had the shortest
training time compared to the other models.
The MLP model had by far the longest training time with over 16 seconds for the relatively small dataset of 400 samples.
Regarding the inference time the Gradient Boosting model was the fastest, while the MLP again was the slowest.
The RF and ET models demonstrated comparable inference times.

In terms of memory usage the MLP model was the most memory intensive model with 170 KB of memory usage.
The other model's memory usage was comparably low ranging from 157 KB to 159 KB.
The dataset's small size leaded less complex models and that contributed to the low memory usage.

\textbf{Hypothesis 3:} ML models with high interpretability can provide valuable insights into the factors that
contribute to spring back, potentially leading to better understanding of the underlying physical processes and
more effective compensation strategies.

To answer the third hypothesis model-agnostic methods were used to evaluate the interoperability, as model specific
method where not suitable since the used interpretable algorithm did not perform well.
Global method agnostic methods such as feature dependence and partial dependence plots where applied to the best
performing models.

Thickness, distance and die opening where all identified as important features for the prediction spring back with the
thickness of the metal sheet having the biggest influence on the outcome.
Partial dependence plots revealed non-linear dependencies between the features and the target variable.
Permutation feature importance was to to further investigate the importance of the features.
The results indicated that thickness and punch penetration were crucial for the models decision-making while while the
die opening was important but only in combination with the other two features.
Two-way partial dependence plots showed an interaction between thickness and distance, confirming the importance of the
die opening for the predictions.

Also the model-agnostic method LIME was applied to different machine learning models but did not yield any new insights.

\subsubsection*{Further Results}
Additionally to the results found for the three hypotheses more insights where gained during the research process.
This study used the \ac{DSR} methodology to guide the research process as well as Design Principles as guidelines for
this process.
It is common for other research to use only regression metrics like MAE and RMSE for to evaluate the models.
This is understandable since the correctness of a models is usually the most important factor for the evaluation.
But these metrics alone do not provide a complete picture of the models performance, since they do not consider
other important factors like the interpretability of the model or the computational resources needed to train and
predict with the model.

This study attempted to create a more comprehensive framework for the evaluation of the models by using the Design
Principles as based on the IEEE ISO/IEC 9126 guidelines.
Until a similar standard is developed for \ac{ML} models this study provides a good starting point for future research
in this field.
This framework could be used in further research to evaluate the performance of \ac{ML} models not only in the field of
spring back prediction but also in other fields.

\subsection{Interpretation of Results}\label{subsec:interpretation-of-results}

The results of the this study provide insights in the applications of machine learning on the field of metal sheet
bending and the prediction of spring back.
The interpretation of the results is as follows:

\subsubsection{Hypothesis 1}
The findings support the first hypothesis that \ac{ML} models can accurately predict the spring back of bent sheet
metal parts.
The Extra Tree and Support Vector Machine models performed best with a RMSE of 0.15 mm, while other models still
exhibited good performance.
This means that in average the predictions of the best models derivative by 0.15 mm from the actual spring back,
with spring backs ranging from 0.5 mm to 3.1 mm in this study this performance is acceptable for many applications.

Traditional trial-and-error methods, such as technology tables (refer to
\cref{sec:problem-identification-and-motivation}), offer data for a limited number of bends, encompassing
only those bends that were previously tested.
In contrast, the resulting \ac{ML} models from this study are capable of predicting an unlimited number of new bends
with high accuracy.
Therefore, it can be conclusively stated that \ac{ML} models are capable of outperforming traditional methods.

In the presentation section (\cref{sec:demonstration}) it could be seen that the models are not perfect.
In particularly they tend to underestimate high spring back values, this could be part of an area where further research
may be needed.
To improve the prediction accuracy more data for high spring back cases might already be enough to improve the to a
satisfying level.
The reason for that is that not enough high spring back cases are present in the dataset as can be seen in
\cref{sec:research-method-and-structure}, consequently the trained ML models do not perform well that regard.

The use of real-word data instead of simulated data for the training of the models make the results of this study more
representative for real-world applications.
A comparison with other studies is challenging since variations in the bending process such as material, machine,
bending method and other factors might yield significantly different results.
%In comparison with similar studies like \cite{baig_machinelearningprediction_2021} it can be seen that the
%real -world data used definitely paid of comparing the results

\subsubsection{Hypothesis 2}

The results show that specific \ac{ML} perform better compared to other models based on the six Design Principles
that where used.
The Extra Tree model demonstrated the best balance between bias and variance, as well as robust performance when
exposed to missing data and noise.
The Gradient Boosting model displayed the highest stability, while the Extra Tree and Random Forest models exhibited
the best resource utilization.
These findings suggest that selecting the most suitable ML models for specific applications is crucial for achieving
optimal performance.

The best overall performance was achieved by the Extra Tree model.
It was the only model that performed well in all performed tests.
The Extra Tree model is a good choice for applications where the model should be trained on a small dataset and
Should be able to accurately forecast new samples.
It is crucial to highlight that only a simple neural network was utilized in this study, and while it produced good
results, more complicated neural networks could produce even better outcomes.
Further more the use of ensemble models could yield interesting results (see recommendations for further
research~\ref{subsec:recommendations-for-further-research}).

\subsubsection{Hypothesis 3}
The results show interpretability methods can provide valuable insights into factors that contribute to spring back,
potentially leading to a better understanding of the underlying physical processes and more effective compensation
strategies.
The thickness of the metal sheet was found to have the most significant influence on spring back, while other
features, such as punch penetration and die opening, also played important roles in the models' decision-making
processes.

With he use of model-agnostic methods, the study also revealed non-linear dependencies between the input and the
target features, as well as interactions between thickness and distance.
This information can be valuable for further refining the models, as well as for developing more effective
compensation strategies in the sheet metal forming process.
This study was not able to use models with high interpretability such as decision trees or linear models
since their performance was not good enough.

\subsection{Limitations of Research}\label{subsec:limitations-of-research}
There are some limitations to this studay that needed to be considered when interpreting the results.

The dataset used has a limited number of samples, with 400 samples the models might not have enough data to capture
the full complexity of the spring back phenomenon.
Furthermore the dataset did not contain many high spring back scenarios, which might have led to the models
underestimating high spring back values.
A larger dataset with more high spring back samples could potentially improve the model performance.
Additionally it has to be noted that many parameters weren't changed in the experimental setup.
For example the used press brake, punch radius and material of the metal sheets where not changed.
Experiments with different bending methods and materials will most certainly yield different results.

This study had a limited number of input features (punch penetration, die opening and
metal sheet thickness).
Other potentially relevant factors such as material properties, temperatures might limited the models ability to
capture all factors affecting the spring back.

Also this study had to selected a limited number of models,
there might be other models or more advanced techniques that might yield better results, recommendations for further
research will be made in the following section.

\subsection{Recommendations for Further Research}\label{subsec:recommendations-for-further-research}
The limitations identified in this study provide direction for future research.

This study selected a limited number of \ac{ML} methods, but there are other models that could be used to
predict spring back.
Based on the results, the \ac{MLP} demonstrated superior performance in multiple design principles, indicating that
deep learning models may be promising candidates for future research.
Furthermore using ensemble models that combine the strengths for multiple other models using bagging or stacking could
lead to interesting new results.
This would involve training multiple other models on the same dataset and combine their predictions to improve the
prediction performance and the robustness of the models.
Additionally the dataset could be expanded by adding more samples with high spring back values.
A larger dataset could potentially improve the model performance.

One of the most interesting directions for future research is the use of \ac{ML} models to predict bend deduction.
Bend deduction is the phenomenon where the metal sheet shrinks during the bending process, resulting in a smaller
piece than the one specified in the design.
Similar to the spring back this can lead to fitting issues and is therefore an important topic for further research.
As noted by~\cite{strano2017fusion}, while some predictions models are available for spring back, bend deduction is
frequently overlooked and could be a fruitful topic for future investigation.

Moreover there are likely other factors that influence the spring back beyond those included as features in this study.
To account for these additional patterns, future research could consider including features such as rolling
direction, temperature, and tension.
