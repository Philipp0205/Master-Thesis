\chapter{Evaluation}\label{ch:evaluation}
% take a step back and put your results from 4 into context. 
This chapter conducts a comprehensive analysis of the \ac{ML} models implemented in
the preceding chapter.
An overview of the quality metrics used for the evaluation is presented in
Table~\ref{tab:evaluation_criteria}.
The quality metrics have been structures based on
the \ac{GQM}approach mentioned in subsection~\ref{subsec:goal-question-metric-approach}.

The quality metrics are aligned with the quality model from~\cite{
    siebert2022construction}, which in turn is based on the ISO/IEC 9126 standard for
software quality evaluation.
The standard has been adapted by \cite{siebert2022construction} to meet the specific
requirements of machine learning models, as described in section~\ref{sec:evaluation-of
-machine-learning-models}.

As~\cite{siebert2022construction} mostly focuses on the evaluation of classification
models, the quality metrics have been adapted to fit the requirements of the regression
models implemented in this thesis.
This is elaborated in each individual quality metric section.

Table~\ref{tab:evaluation_criteria} shows the quality metrics used for the evaluation
of the \ac{ML} models.
The \ac{GQM} approach is used to structure the evaluation of the \ac{ML} models.
To each goal a question or one or more metrics where assigned.
The following subsections follow this structure and describe the quality metrics in more
detail.

\captionsetup{margin={5pt,5pt}}
\begin{table}[H]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \sisetup{group-minimum-digits = 4}
        \centering
        {\renewcommand{\arraystretch}{1}
            \begin{tabular}{p{2cm}p{8cm}p{3cm}}
                \toprule
                \thead{\textbf{Goal}} & \thead{\textbf{Question}}
                & \thead{\textbf{Metric}} \\
                \toprule
                \textbf{Correctness} & Ability of the model to perform the
                current task measured on the development dataset and the runtime dataset
                \cite[p. 16]{siebert2022construction}
                &
                MAE, \newline MSE, \newline RMSE
                \\
                \hdashline
                \textbf{Relevance} & Does the model achieve a good bias-variance
                tradeoff? Which means neither overfitting or unterfitting the
                data.
                \cite[p. 16]{siebert2022construction}
                & Variance of CV, \newline $R^2
                \\
                \hdashline
                \textbf{Robustness} & Ability of the model to outliers, noise
                and other data quality issues
                \cite[p. 16]{siebert2022construction}
                & Loss of Accuracy, \newline Average Loss
                \\
                \hdashline
                \textbf{Stability} & Does the artifact generate repeatable
                results when trained on different data?
                \cite[p. 16]{siebert2022construction}
                & LOOCV stability
                \\
                \hdashline
                \textbf{Interpret- ability} & How well can the model be
                explained?
                \cite[p. 16]{siebert2022construction}
                & Linearity, \newline monotonicity, \newline  interaction
                \\
                \hdashline
                \textbf{Resource utilization} & How much resources are
                required to train and run
                the model?
                \cite[p. 16]{siebert2022construction}
                & Training time, \newline runtime, \newline storage space
                \\
                \bottomrule
            \end{tabular}
        } % renew command
    \end{tcolorbox}
    \caption{Overview of the goals, questions and metrics for the
    evaluation of artifacts
    following the \ac{GQM} approach.}
    \label{tab:evaluation_criteria}
\end{table}


\section{DP1: Correctness}\label{sec:dp1:-correctness}
% Ability of the model to perform the current task measured on the
% development dataset and the
% runtime dataset

The model must be able to perform well on the selected task.
\cite{siebert2022construction} used the classification metrics precision, recall and F-
score to evaluate the correctness models.
As the regression models implemented in this thesis are not able to predict
the class of a sample, these metrics are not applicable.

Commonly used metrics for regression models are the mean absolute error \ac{MAE},
\ac{MSE} and \ac{RMSE}.
Their formal description is given in~\ref{eq:mae},~\ref{eq:mse} and~\ref{eq:rmse} where
\(e_i\) being the prediction error which is the difference between the predicted value
by the model and the acutal value.
\(y_i\) is the actual value and \(n\) is the number of samples in the testing data set.

For the evaluatoins the \texttt{sci-kit learn}~(\cite{scikit-learn}) implementation of
these metric was
used.

\textbf{Mean Absolute Error (MAE)}

\begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
    \begin{equation}
        MAE = \frac{1}{n} \sum_{i=1}^{n} |e_i|
        \label{eq:mae}
    \end{equation}
\end{tcolorbox}

The MSE is the average of the squared differences between the predicted and
the actual values.
The MSE is more sensitive to outliers than the MAE.

\textbf{Mean Squared Error (MSE)}

\begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
    \begin{equation}
        \label{eq:mse}
        MSE = \frac{1}{n} \sum_{i=1}^{n} e^2
    \end{equation}
\end{tcolorbox}

The RMSE is the square root of the MSE. The RMSE is the most popular metric
for evaluating the
performance of regression models. The RMSE is interpretable in the same units
as the response
variable. The RMSE is more sensitive to outliers than the MAE.
The MAE is the average of the absolute difference between the predicted and
actual values.

\textbf{Root Mean Squared Error (RMSE)}

\begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
    \begin{equation}
        \label{eq:rmse}
        RMSE = \sqrt{MSE}
    \end{equation}
\end{tcolorbox}

For a full overview about the performance all three metrics are sued for the
evaluation.

% \subsection{Goal Hierarchy}

% The correctness describes the ability of the model to perform on the
% selected task. Concrete,
% the model has to predict the spring back as correct as possible. The goal
% hierarchy for the
% correctness is shown in Figure~\ref{fig:goal_hierarchie_correctness}.

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.6\textwidth]{goal_hierarchie_correctness}
%     \caption{Goal hierarchy: Correctness}
%     \label{fig:goal_hierarchie_correctness}
% \end{figure}

\subsection{Results}\label{subsec:results}
The Table~\ref{tab:results-correctness} shows the results of the
evaluation of the models for the correctness.

Looking at the \ac{RMSE} all models except the \ac{LR} and \ac{DC} models show a
relatively equal performance.
The \ac{MLP} has a slight advantage over the other models, with an \ac{RMSE} of 0.201.
Therefore it could be interesting to investigate the performance of neural networks /
deep learning in the future.

It's worth mentioning that the \ac{SVM} model has a slightly better \ac{MAE}, while
still maintaining a comparable \ac{MSE} compared to the other models.
This could be due to the fact that the errors made by the \ac{SVM} are evenly
distributed across instances, or that the errors made by the other models are large for
a few instances but smaller for the rest.

The algorithms \ac{LR} does perform worse than the other models.
A likely reason is, that the relationship between the independent and dependent
variables is not linear.
This cna result in an inadequate fit and reduced predictive power (Source).

\ac{DT} on the other hand, can perform poorely when they overfit the training data.
Also, they are pron to instability, both problems are mitigates by using ensemble methods
like \ac{RF} and \ac{GBT} which can overcome the limitations of individual
decision trees.

Because the simple \ac{DT} as well as the \ac{LR} are not able to predict the
spring back correctly, they are not considered for the next steps of the evaluation.

% Table wit hall used machine learning models and their metrics
\begin{table}[H]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
% \sisetup{group-minimum-digits = 4}
        \centering
        \begin{tabular}{llll}
            \toprule
            \thead{\textbf{Model Name}} & \thead{\textbf{MAE}}
            & \thead{\textbf{MSE}}
            & \thead{\textbf{RMSE}} \\
            \toprule
            \textbf{LR}  & 0.199 & 0.092 & 0.303 \\
            \hdashline
            \textbf{DT}  & 0.193 & 0.076 & 0.266 \\
            \hdashline
            \textbf{RF}  & 0.183 & 0.060 & 0.244 \\
            \hdashline
            \textbf{ET}  & 0.126 & 0.040 & 0.201 \\
            \hdashline
            \textbf{AB}  & 0.163 & 0.040 & 0.209 \\
            \hdashline
            \textbf{GBT} & 0.189 & 0.058 & 0.242 \\
            \hdashline
            \textbf{SVM} & 0.09  & 0.04  & 0.20  \\
            \hdashline
            \textbf{MLP} & 0.114 & 0.041 & 0.201 \\
            \bottomrule
        \end{tabular}
        \caption{Overview of the used machine learning models and their
        metrics.}
        \label{tab:results-correctness}
    \end{tcolorbox}
\end{table}


\section{DP2: Relevance}\label{sec:relevance}
% Does the model achieve a good bias-variance tradeoff? Which means neither
% overfitting or
% unterfitting the data.

% Bias variane trade-off
A model is considered relevant when it achieves a balance between bias and
variance, avoiding both overfitting and underfitting of the training data.
The relevance of the model can be quantified through the \textit{variance of
cross-validation}, which proves insight into how the model performs when trained and
evaluated on different subsets of data and how generalizes.

% Explanation interpretation of variance
A low variance indicates that the model's performance is consistent across
different folds,suggesting that the model is not overfitting the training data.
Conversely, a high variance implies that the performance can vary significantly
depending on the specific data points used the test set, indicating a potential
overfitting problem.

Figure~\ref{fig:variance-of-cv} shows how the variance of cross-validation
was calculated.
The parameter \(E\) denotes to the estimator which is used to calculate the
\ac{CV} score.
The parameter \(k\) denotes to the number of folds, which was set to 5 in this
context.
As estimator for the trained models \(R^2\) was used.
The \(R^2\) is a statistical measure of how close the data are to the fitted regression
line.

\begin{figure}[h]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \centering
        \includegraphics[trim=left botm right top, width=0.9\textwidth,
            clip]{cross_validation}
        \caption{Variance of cross-validation}
        \label{fig:variance-of-cv}
    \end{tcolorbox}
\end{figure}

\begin{equation}
    \label{eq:r2}
    R^2 = \frac{Explained\_variance}{Total\_variance\_targert\_variable}
\end{equation}

The value of the \(R^2\) is the percentage of the response variable variation that is
explained by a linear model.
It returns a score between 0 and 1, where 0 means that the model does not
explain any of the variance in the response variable around its mean, and 1 means that
the model explains all the variance in the response variable around its mean~\cite[p.
43]{muller_introductionmachinelearning_2016}.
Special in sci-kit learns implementations is that the score can be negative if the model
performs poorly. If a model always predicts the expected value of y, regardless of the
input features, it would receive an R^2 score of 0.0~\cite{_sklearnmetricsr2_}.

Therefore, a high \(R^2\) indicated a good model fir and good bias-variance
tradeoff\cite[p. 43]{muller_introductionmachinelearning_2016}.

\subsection{Results}\label{subsec:results3}

Table~\ref*{tab:results_relevance} shows the variance of cross-validation
and the \(R^2\) for all used machine learning models.
To calculate the variance of cross-validation the variance of Scikit-Learn's
\texttt{cross\_val\_score} was calculated.
Five-fold cross-validation was used to calculate the variance of
cross-validation.
The \(R^2\) was calculated with the formula~\ref{eq:r2}.

It can be observed that most of the models have relatively low variance, indicating
consistent performance across different folds, however, two models stand out - \ac{GBT}
and ac{MLP}.

For the \ac{GBT} model, the variance is high, which implies that its performance
fluctuates significantly across different folds.
This inconsistency is a typical sign of overfitting, where the model is overly complex
and has learned the noise in the data rather than the underlying pattern.
The individual cross-validation scores for the GBT model also confirm this assumption,
with a negative score on the first fold and relatively high scores on the subsequent
folds.
Given this poor performance, the GBT model cannot be considered relevant for the given
task.

Similarly, the \ac{MLP} model has a relatively high variance score, which suggests that
its performance also fluctuates across different folds. The individual scores for the MLP
model show that it performs poorly on a few folds.

This performance inconsistency could be due to either overfitting or high variability
in the data.

Overall, it's important to consider the cross-validation scores in conjunction with
the other other metrics to get a more complete picture of how well your model is
performing.
This will be done in later sections.

\begin{table}[H]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
% \sisetup{group-minimum-digits = 4}
        \centering
        \begin{tabular}{llll}
            \toprule
            \thead{\textbf{Model Name}} & rs & \thead{\textbf{Variance of CV}}
            & \thead{\textbf{\(R^2\)}} \\
            \toprule
            \textbf{RF}  & - & 0.068 & 0.8   \\
            \textbf{RF}  & x & 0.071 & 0.805 \\
            \hdashline
            \textbf{ET}  & - & 0.064 & 0.832 \\
            \textbf{ET}  & x & 0.352 & 0.82  \\
            \hdashline
            \textbf{GBT} & - & 0.425 & 0.775 \\
            \textbf{GBT} & x & 0.426 & 0.866 \\
            \hdashline
            \textbf{SVM} & - & 0.061 & 0.838 \\
            \textbf{SVM} & x & 0.061 & 0.798 \\
            \hdashline
            \textbf{MLP} & - & 0.146 & 0.839 \\
            \textbf{MLP} & x & 0.146 & 0.871 \\
            \bottomrule
        \end{tabular}
        \caption{Performance of the used machine learning models according
        relevance.}
        \label{tab:results_relevance}
    \end{tcolorbox}
\end{table}


\section{DP3: Robustness}\label{sec:robustness}
% Ability of the model to outliers, noise and other data quality issues
% Variance of cross-validation, fit

%It should be noted that the original definition of the ``Robustness`` metric was limited
%to classification models.
%To address this limitation, the definition of the metric has been altered to fit the
%requirements of the regression models implemented in this thesis. This is further
%described
%in the section ``Robustness``~\ref{sec:robustness}.

% Also sibert et all uses Equalized Loss of Accuracy (ELA) which was changed

% Definition of robustness
The IEEE standard glossary of software engineering terminology describe
robustness as ``The degree
to which a system or component can function correctly in the presence of
invalid inputs or
stressful environmental conditions''~\cite[p. 64]{terminology1990ieee}.
\cite{saez2016evaluating} extend this definition to fit machine learning
models and describe it
as the ``capability of an algorithm to build models that are insensitive to
data corruptions and
suffer less from the impact of noise''~\cite[p.
2]{saez_evaluatingclassifierbehavior_2016}.
\cite{siebert2022construction} specifically mention data quality issued like
outliers or noise~\cite[p. 16]{siebert2022construction}.

Unlike \ac{DP} Correctness(\ref{sec:dp1:-correctness}), robustness is a
non-functional characteristic of a \ac{ML} model.
A way to evaluate the robustness is to check the correctness of the model with
added noise to the data~\cite[p. 18]{zhou_machinelearning_2021}, if the model
is still able to predict the correct values, it is considered robust.

As mentioned in section~\ref{subsec:dataset-exploration} the data set
was generated in a controlled environment does not contain many data quality
issues and noise.
This offers the opportunity to test the robustness of the models by manipulating the
data set to introduce the data quality issues missing values and noise.

\subsection{Missing Data}\label{subsec:missing-data}
When applying the model to real-world data, it is possible that some values
are missing.
Looking at the available dataset two scenarios or missing data can occur:
Missing \(Vt\)-pairings and missing values.

In the first scenario, there may be no data available for a specific die
opening, which can be due to the die opening was never being used before or a
lack of recorded
information.
In the second scenario, while data may exist for the desired \(Vt\)
combination, certain values may
be missing or the dataset may not be complete.

To address these scenarios, the following tests were conducted:

\subsubsection{Missing values}
%To assess the ability of the model to handle missing data, three experiments were
%conducted.

Initially, the use of the \ac{LPOCV} method was considered.
However, this approach was omitted due to computational constraints, as it generates all possible
training and test sets by removing \(p\)samples from the complete dataset, resulting in a large
number of overlapping test sets and a high computational cost.

As an alternative, regular \ac{CV} was applied, with an increasing number of folds.
The minimum number of folds was 2 and the maximum was equal to the total number of samples in the
dataset.
However, this experiment did not provide distinguishable results and was therefore not
included in the results.

The third and final experiment utilized random sampling to create different train-
test data compositions, allowing for the evaluation of the model's handling of missing data.
Split rations from 90:10 to 10:90 were used, with a step size of 10. This approach was inspired by a
similar method used in a previous study~\cite[p. 570--574]{liu2021deep}  which used thee different
split ratios to evaluate the performance of a deep learning model.


The results of this experiment provided valuable insights into the model's performance under
different data compositions, particularly when dealing with missing
data, and were used in conjunction with other metrics to gain a comprehensive understanding of
the model's overall performance.

%\begin{table}[H]
%    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
%% \sisetup{group-minimum-digits = 4}
%        \centering
%        \begin{tabular}{lllll}
%            \toprule
%            \thead{\textbf{Model }} & rp &
%                {\thead{\textbf{80\% train} \\ \unit{MSE}}} &
%                {\thead{\textbf{50\%train} \\ \unit{MSE}}} &
%                {\thead{\textbf{20\%train} \\ \unit{MSE}}}
%            \\
%            \toprule
%            \textbf{RF} & - & 0.106 & 0
%            .099 & 0.012 \\
%            \textbf{RF} & x & 0.064 & 0
%            .089 & 0.114 \\
%            \hdashline
%            \textbf{ET} & - & 0.75 & 0
%            .116 & 0.094 \\
%            \textbf{ET} & x & 0.059 & 0
%            .093 & 0.079 \\
%            \hdashline
%            \textbf{GBT} & - & 0.045 & 0
%            .055 & 0.069 \\
%            \textbf{GBT} & x & 0.045 & 0
%            .055 & 0.069 \\
%            \hdashline
%            \textbf{SVM} & - & 0.071 & 0
%            .086 & 0.074 \\
%            \textbf{SVM} & x & 0.071 & 0
%            .086 & 0.074 \\
%            \hdashline
%            \textbf{MLP} & - & 0.053 & 0.057 & 0.45 \\
%            \textbf{MLP} & x & 0.053 & 0.057 & 0.54 \\
%            \bottomrule
%        \end{tabular}
%        \label{tab:ml_models_relevance}
%    \end{tcolorbox}
%    \caption{Performance of the used machine learning models according
%    relevance.}
%\end{table}

Figure~\ref{fig:results-missing-values} shows how the models performed when trained on
continuously less data.

It can be seen that all models perform well when trained with on 90\% of the data.
However, when trained on 50\% of the data, the \ac{MLP}  and \ac{GBT} models start to perform
better than the
other models.
When trained on 20\% of the data and less all models significantly underperform.
Also the \ac{ET} model stays relatively stable when trained on less data.

Looking at the variances the the \ac{SVM} and \ac{MLP} models perform the and seem to be the most
robust models when trained on less data.
It has to be noted, that as seen in sub-figure (a) the most variances comes from training on
below 20\% of the data.

Overall the \ac{MLP} model seems to be the most robust model, as it performs well on all data
compositions and has the lowest variance.

\begin{figure}[H]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \centering
        \begin{subfigure}{0.4\textwidth}
            \includegraphics[width=\textwidth]{missing_values_plot}
            \caption{}
            \label{fig:first}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.4\textwidth}
            \includegraphics[width=\textwidth]{variance_missing_values}
            \caption{}
            \label{fig:second}
        \end{subfigure}
        \hfill
        \label{fig:results-missing-values}
    \end{tcolorbox}
    \caption{Figure (a) shows the comparison of performance on less data. Figure (b) shows the
    variance of the models when trained on less data.}
\end{figure}

%\begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
%    \begin{equation}
%        \label{eq:average_loss}
%        \text{Loss of Accuracy} = \frac{1}{N} \sum_{i=1}^{N} |\text{RMSE}i -
%        \text{RMSE}{avg}|
%    \end{equation}
%\end{tcolorbox}

%Where \(N\) is the toal number of folds, \(i\) is the index of the current folds.
%
%It is worth noting that imputations of missing values can be used to mitigate
%the loss
%caused by missing data.
%However, this approach was not used in this thesis as the goal
%was to measure the robustness of the model to missing values specifically,
%rather than to
%artificially improve its performance through imputation techniques.

\subsection{Noise}\label{subsec:noise}
A common practise in \ac{ML} is to add noise to the training data to improve
the model's generalization abilities.
In the following section Gaussian Noise will be added to the data set to evaluate the robustness
of the model.

There is no standard approach to adding noise to the data.
In this thesis, the noise was added to the training data only.

To add the noise, \textit{numpy.random.normal} function was used, which
generate random numbers with a Gaussian distribution~\cite{scikit-learn}.
The mean and standard deviation of each feature were calculated based on the
original data, and the function was used to generate noisy values for each feature.

To study the model's reaction to increasing amounts of noise, the noise added
to the training data was gradually increased starting from 1\% to 50\%, which means that the
last iteration contains as many noisy as ``clean'' samples.

The difference between all \ac{RMSE} between iterations was defined as loss.
The total loss was calculated by averaging the difference of these values as shown in Equation~\ref{eq:noise}.

\begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
    \begin{equation}
        \text{Average Loss} = \frac{1}{N} \sum_{i=1}^{N} |\text{RMSE}i -
        \text{RMSE}{avg}|\label
        {eq:noise}
    \end{equation}
\end{tcolorbox}

\begin{itemize}
    \item \textit{Does the calculation of the loss make sense?}
\end{itemize}

\subsection{Results}\label{subsec:results-robustness}

\begin{table}[H]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
% \sisetup{group-minimum-digits = 4}
        \centering
        \begin{tabular}{llll}
            \toprule
            \thead{\textbf{Model Name}} & rs & {\thead{\textbf{Noise} \\ \unit{loa}}}
            \\
            \toprule
            \textbf{LR} & - & 0.070 & 0.251 \\
            .071 & 0.224 \\
            \hdashline
            \textbf{Random Forest} & x & 0.241 \\
            \hdashline
            \textbf{SVM} & - & 0.251 \\
            \hdashline
            \textbf{MLP} & - & 0.451 \\
            \bottomrule
        \end{tabular}
        \caption{Results of used machine learning models regarding the design
        principle robustness.}
        \label{tab:results_robustness}
    \end{tcolorbox}
\end{table}


\section{DP4: Stability}\label{sec:stability}
% Does the artifact generate repeatable results when trained on different data?
% Leave-one-out cross-validation stability 
Stability is defined as the ability of the model to generate repeatable
results when trained on different data~\cite[p. 16]{siebert2022construction}.

One appropriate way to measure the stability of a model is to use \ac{LOOCV},
which is a a form of \ac{CV} where one sample is used for validation and the remaining
data is used for training~\cite[p. 200--201]{gareth2013introduction}.
LOOCV is a technique where the observation set is divided into two parts, but instead of having
two subsets of comparable size, one sample is used for validation, while the rest form the
training set.

In order to evaluate the model the \ac{LOOCV} was repeated for all samples in
the dataset resulting in a total of \(n\) iterations.
The stability of the model was determined by calculating the average prediction error across all
iterations, using the equation provided in Equation~\ref{eq:loocv} which is taken from~\cite[p.
201]{gareth2013introduction}.

\begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
    \begin{equation}
        CV_{(n)} = \frac{1}{n} \sum_{i=1}^{n} \text{MSE}_{i}\label{eq:loocv}
    \end{equation}
\end{tcolorbox}

The resulting \ac{MSE} is a poor estimate of the model's generalization error because only one
sample is used for validation~\cite[p. 201]{gareth2013introduction}, but it can be used to make a
statement about the stability.

The stability of the model can be evaluated by examining the standard derivation of the cross
validations scores.
A low standard derivation across all folds indicated that the model can generated consitent
results when trained on different datasets, suggesting are more sable model.

It is important to note that there are many different ways to measure the modelstability.
In this work LOOCV was chosen for three reasons.
%Another common approach is to use regular Cross-Validatoin, there are three reasons why LOOCV was
%chosen for this work.
Firstly, \ac{LOOCV} uses nearly all available data for training, while K-fold CV resevers a
portion o f the data for testing.
as the dataset used is relatfively small, it is an advantage to use all data for training.
Secondly, \ac{LOOCV} tends to have a lower bias due to the larger training set.
Lastly, multiple approaches have been tried to measure the model stabilty, and LOOCV delivered
he most stable and interpretable results.
The main drawback of the chose method is that it is computationally expensive as it required n
iterations.

Table~\ref{subsec:results-stability} shows the cross-validation score and the standard derivation
of each model. The focus relied more on the standard derivation, as it is a better indicator of
the stability of the model. As already mentioned the \ac{CV} score with using LOOCV is a poor
estimator.

---

\subsection{Results}\label{subsec:results-stability}

\begin{table}[H]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
% \sisetup{group-minimum-digits = 4}
        \centering
        \begin{tabular}{ll}
            \toprule
            \thead{\textbf{Model Name}} & \thead{\textbf{std of CV scores (MSE)}}
            \\
            \toprule
            \textbf{Random Forest}          & 0.047 (0.891) \\
            \textbf{Random Forest}          & 0.200 (0.047) \\
            \hdashline
            \textbf{Gradient Boosting}      & 0.044 (0.922) \\
            \textbf{Gradient Boosting}      & 0.190 (0.039) \\
            \hdashline
            \textbf{Extra Trees}            & 0.46  (0.877)   \\
            \textbf{Extra Trees}            & 0.199 (0.053) \\
            \hdashline
            \textbf{Support Vector Machine} & 0.050 (0.853) \\
            \textbf{Support Vector Machine} & 0.266 (0.061) \\
            \hdashline
            \bottomrule
        \end{tabular}
    \end{tcolorbox}
    \caption{Results of the stability of the models.}
    \label{tab:results-stability}
\end{table}


\section{DP5: Resource utilization}\label{sec:resource-utilization}
% How much resources are required to train and run the model?
% Training time, runtime, storage space

To measure the resource utilization of the model, the following metrics are
used:

% From Copilot
\paragraph*{Training time}
Measured in seconds. Refers to the time it takes to train the model.
Training a model requires resources such as memory, CPU, and GPU, therefore
the longer it takes
to train a model, the more resources are required. According to resources
utilization a shorter
training time is desirable.

The training time is measured using the \texttt{time.time} function in python
. The function
returns the time in seconds since the epoch. The time is measured before and
after the model is
fitted. The difference between the two times is the training time.

\textbf{Inference time}
Measured in milliseconds. It refers to the time it takes to make a prediction
on data once it has
been trained.
It is an important measure not only for real-world application but also a
faster runtime uses
less resources and is therefore more efficient.

\textbf{Inference time}
Measured in milliseconds. Time it takes to make 100 predictions.
A model that takes longer to make predictions may be more complex than a
model that is able to
make predictions more quickly.
Measured using the \texttt{time.time} function in python. 100 valuees are
picked out of the teset
set and the time is measured before and after the prediciton is made.

\textbf{Memory space}
Measured in Mb.
It refers to the amount of memeory required to run the model.
The more storage space required to store the model, the more resources are
required to store it.
Therefore, a smaller storage space is desirable.
To measure the memory usage the \texttt{memory\_usage} function from the
\texttt{memory\_profiler}
package is used.


\begin{table}[H]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
% \sisetup{group-minimum-digits = 4}
        \centering
        \begin{tabular}{lllll}
            \toprule
            \thead{\textbf{Model Name}} & {\thead{\textbf{rs}}} & {\thead{\textbf{
                Training
                time} \\
            \unit[]{ms}}}
            & {\thead{\textbf{Inference time} \\ \unit[]{ms}}} &
                {\thead{\textbf{Memory
            Usage} \\
            \unit{kb}}}
            \\
            \toprule
            \textbf{LR} & - & 3.14 & 0.982 & 269.887 \\
            \textbf{LR} & x & 3.558 & 1.157 & 269.887 \\
            \hdashline
            \textbf{DT} & - & 2.743 & 1.136 & 174.0 \\
            \textbf{DT} & - & 3.455 & 1.212 & 174.008 \\
            \hdashline
            \textbf{RF} & - & 25.205 & 2.886 & 173.219 \\
            \textbf{RF} & x & 26.821 & 2.269 & 173.266 \\
            \hdashline
            \textbf{ET} & - & 19.62 & 1.725 & 173.121 \\
            \textbf{ET} & x & 20.854 & 1.887 & 173.156 \\
            \hdashline
            \textbf{SVM} & - & 309.721 & 2.789 & 195.734 \\
            \textbf{SVM} & x & 310.018 & 2.793 & 195.746 \\
            \hdashline
            \textbf{MLP} & - & 19100.697 & 11.663 & 180.949 \\
            \textbf{MLP} & x & - & 19100.697 & 11.663 & 180.949 \\
            \bottomrule
        \end{tabular}
        \caption{Overview of the used machine learning models and their
        metrics.}
        \label{tab:resutls_resource_utilization}
    \end{tcolorbox}
\end{table}

As can be seen in Table~\ref{tab:resutls_resource_utilization}, the \ac{ET} is faster
than to th conventional \ac{RF}.
This expected since the most-time intensive part of the \ac{RF} is the splitting of the
nodes with determining the optimal threshold for each feature at each node
(see section~\ref{subsubsec:extra-trees}).
It has been observed, that this advantage is lost when the \texttt{n\_estimators}
parameter is set above 10.

The \ac{MLP} is the slowest model to train and to make predictions.
Most certainly this is because an \ac{MLP} typically has more parameters to learn and
has a deeper architecture, which means that it requires mor computations to train and
predict.
Also the training algorithm used is backpropagation, wich can be more computationally
intensive.
Also an \ac{MLP} trains all data at once and therefore cant parallelize the process.
It has to be noted, that tuning the hyperparameters is a time-consuming steps and the
chosen parameters might not be the optimal ones.


\section{DP6: Interpretability}\label{sec:interpretability}
% How well can the model be explained?
% Complexity measures (e.g., no. of parameters, depth)

% Sieber et al used
% %
%& Complexity measures (e.g., no. of parameters, depth) \\
% These metrics where changed
When following definitions of interpretability used in
section~\ref{sec:objectives-of-a-solution}, it is clear that it is not possible to
measure interpretability in a quantitative way, but other metohds can beu sed to measure the
interpretability of a model.
%Interpretable models allow the usage of global model-agnostic evaluation
%methods which will be used later in the evaluation of the models (see
%Section~\ref{sec:evaluation}).


According to~\cite{molnar2020interpretable} one way to make a model interpretable is to limit the
choice of algorithms to those that produce interpretable results.
Example of such algorithms include linear regression, logistic regression and decision
trees~\cite[p. 35]{molnar2020interpretable}.

\cite{molnar2020interpretable} defines three properties of interpretable models: linearity,
monotonicity and interactions.
A linear model expresses the relationship between features and the target outcome aa a linerar
equation~\cite[]{molnar2020interpretable}.
Monotonocitiy constraints ensure that there is a consistent relationship between a feature and
the target outcome across the etnire renage of the feature. This can make it easier to understand
the relationship.
Some models automatically include interactions between features to improve prediciotn, while
other require the manual creation of interaction features. However, inlcuding too many or complex
interactions can make the model more difficult to interpret~\cite{molnar2020interpretable}.

In the model selection for this thesis the focuses mostly on interpretable models,
as they are easier to understand and explain, therefore mostly models are chosen which fulfill
all three properties.

Later in the thesis, the interpretability of the models is used to explain
the results.

\subsection{Interpretable Models}\label{subsec:interpretable-models}
Linear Regression and Decision Trees are considered as interpretable models and offer good
model-specific explanations by default~\cite{molnar2020interpretable}.
As mentioned in section~\ref{sec:dp1:-correctness}, both algorithms perform not good enough
to be considered as a solution for the problem.
Therefore the focus is on model agnostic methods to explain the predictions of the models.


\subsection{Model Specific Interpretability
Methods}\label{subsec:model-specific-interpretability
-methods}

To attain interpretability, the simplest approach is to select algorithms
that generate
interpretable models.
Some of these models like linear regression and decision trees where used in
this work.

The following section looks at the generated and uses model specific
interpretability methods to
interpret the results.

\subsubsection{Feature Importance}

Figure~\ref{fig:rf_feature_importance} compares and visualizes the relative
importance of the features used for training the model.
As shown, the thickness is the most important feature followed by distance
and die opening.
The results shows, that all three featured are relevant for the outcome and so no feature can be
removed from the dataset to get a better performance of the model.

\begin{figure}[H]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \centering
        \includegraphics[width=0.6\textwidth]{Random_Forest_Regression_importances}
        \caption{Relative feature importance}
        \label{fig:rf_feature_importance}
    \end{tcolorbox}
\end{figure}

\subsection*{Global Model-Agnostic Methods}
As already described in section~\ref{sec:interpretability}, only algorithms
that are
interpretable where used for the evaluation.

\cite{molnar2020interpretable} differentiates between
model-specific and model-agnostic interpretability methods.
Model-specific methods are methods that are specific to a certain model type, for example,
decision trees.
Model-agnostic methods are methods that can be used with any model type, for
example, permutation importance.

Also \cite{molnar2020interpretable} differentiates between global and local
interpretability. Global
interpretability refers to the ability to understand the overall behavior of
the model, while
local interpretability refers to the ability to understand the behavior of
the model for a
specific instance.

For the purpose of this thesis, the focus is on global model-agnostic
methods, as they can be used
with any model type and provide an overview of the model's behavior.


%\subsubsection*{Number of Parameters}
%More complex models tend to have more parameters and are therefore more
%difficult to interpret.
%The number of parameters is calculated using the the \texttt{model
%.get\_params} function
%in scikit-learn.
%
%% EffiecientnetPaper?
%Note: The Depth of the model could be used as well for the evaluation, but
%not all models have a
%depth therefore this metric is not suitable.
%The depths is representing the number of layers in the model.
%A model with many layers, is generally more complex than a shallow network
%with fewer layers.
%
%\subsubsection*{\textit{Notes}}
%
%\begin{itemize}
%    \item \textit{Space complecity -> How much memeory is needed?}
%\end{itemize}
%
%\subsubsection*{Linear Regression}
%Linear regression is a linear model as it models the relationship between the
%features and the
%target as a linear equation.
%
%This linearity usually does mean that the relationship between the feature
%and the target goes in
%the same direction over the entire range of the feature. Therefore \ac{LR} is
%monotonic.
%
%\ac{LR} models do not include interaction features by default.
%To capture interaction effects in \ac{LR}, interaction features need to be
%created and added to
%the model manually. This was not done in this work.
%
%\subsection{Results}\label{subsec:results2}
%
%\begin{table}[H]
%    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
%% \sisetup{group-minimum-digits = 4}
%        \centering
%        \begin{tabular}{llll|l}
%            \toprule
%            \thead{\textbf{Model Name}} & \textbf{\thead{Linear}} &
%            \textbf{\thead{Monotone}} &
%            \thead{\textbf{Interaction}}
%            & \thead{\textbf{Param.}}
%            \\
%            \toprule
%            \textbf{Linear Regression}   & Yes & Yes  & No  & 4  \\
%            \hdashline
%            \textbf{Decision Tree}       & No  & Some & Yes & 11 \\
%            \hdashline
%            \textbf{Random Forest}       & No  & Some & Yes & 17 \\
%            \hdashline
%            \textbf{Logistic Regression} & No  & Yes  & No  & XX \\
%            \hdashline
%            \textbf{SVM}                 & X   & X    & XX  & 11 \\
%            \hdashline
%            \bottomrule
%        \end{tabular}
%        \caption{Overview of the used machine learning models and their
%        metrics.}
%        \label{tab:interpretable-models}
%    \end{tcolorbox}
%\end{table}
%
%\subsubsection*{Notes}
%\begin{itemize}
%    \item \textit{Reasoning for results in table.}
%\end{itemize}

\textit{Explanation of the results}

\subsection{Overall Results}\label{subsec:overall-results}
This section will showcase the outcomes generated by the trained models in more detail and
will also discuss the results in relation to the research questions.

In Section~\ref{sec:dp2:-correctness} the overall correctness of the model was
evaluated using different metrics.
In order to evaluate the results of this problem in more detail, three test cases are used
in this analysis.
The V/t ratio is used to determine the bending conditions of the test cases. In the
dataset used in ranged from 3.3 (\(V 50\)and \(t 3\))to 100 (V50 t 0.5)and the test
cases are chosen to represent the different bending conditions.

Recommended V/t ratios in industrial practices are between 6 and 10~\cite[p.7]{
    cruz_applicationmachinelearning_2021}.
Bending operations performed outside of this range of recommended ratios may result in
high spring back.

Case A is chosen, so that is is below the recommended ration, case B is chosen to be
within the recommended ratios and case C is chosen to be above the recommended ratios.
As can be seen, case A features a significant spring back while the other cases do not.

In order to evalute the model's performance without bias, each test case where removed
from
the training data set so that the model has not seen the test case before.
This results in a small but notable performance decrease in the test cases.
%
%The chosen test cases are
%\begin{itemize}
%    \item V20 t3 (6.6) (for now)
%    \item V30, t1.5 (15)
%    \item V50 t0.5 (100)
%\end{itemize}

Case \ref{fig:performance-20-1} with a V/t-ratio of 20 demonstrates the models'
ability to predict the spring with high accuracy.
However, the performance decreases in case b)\ref{fig:performance-20-3} with a V/t-ratio
of 6.6, particularly when predicting the spring backs.
It is likely that there is a measurement error at V = 7.5 in case
b)~\ref{fig:performance-20-3}, causing some models to potentially predict the correct
spring back.

\begin{figure}[H]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \begin{subfigure}{0.5\textwidth}
            \includegraphics[width=\textwidth]{performance_20_1}
            \caption{V: 20, t: 1}
            \label{fig:performance-20-1}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.5\textwidth}
            \includegraphics[width=\textwidth]{performance_20_3}
            \caption{V: 20, t: 3}
            \label{fig:performance-20-3}
        \end{subfigure}
    \end{tcolorbox}
    \label{fig:performance-case-a}
    \caption{Performance plots for case A}
\end{figure}

As seen in Figure~\ref{fig:performance-case-b}, the performance in case B is similar.
The models exhibit high accuracy in predicting the spring back in case a)~\ref{fig
:performance-30_1.5} with a V/t-ratio of 15. Additionally, the spring backs at the
higher end are also predicted with high accuracy.

\begin{figure}[H]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \begin{subfigure}{0.5\textwidth}
            \includegraphics[width=\textwidth]{performance_30_1.5}
            \caption{V: 30, t: 1.5}
            \label{fig:performance-30_1.5}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.5\textwidth}
            \includegraphics[width=\textwidth]{performance_30_2.0}
            \caption{V: 30, t: 2}
            \label{fig:performance-30_2.0}
        \end{subfigure}
    \end{tcolorbox}
    \caption{Performance plots for case B}
    \label{fig:performance-case-b}
\end{figure}

In case C as seen in Figure~\ref{fig:performance-case-c} the models performance is
getting worse.

Reasons for that? ...

\begin{figure}[H]
    \begin{tcolorbox}[arc=0pt,boxrule=0.5pt]
        \begin{subfigure}{0.5\textwidth}
            \includegraphics[width=\textwidth]{performance_50_0.5}
            \caption{V: 30, t: 1.5}
            \label{fig:performance-30_1.5}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.5\textwidth}
            \includegraphics[width=\textwidth]{performance_50_1}
            \caption{V: 30, t: 2}
            \label{fig:performance-30_2.0}
        \end{subfigure}
    \end{tcolorbox}
    \caption{Performance plots for case C}
    \label{fig:performance-case-c}
\end{figure}


The best performing model is the \ac{SVM} model with an \ac{RMSE} of 0.2.
Therefore the cases A, B and C are predicted using the \ac{SVM} model.

As seen in Figure~\ref{fig:default}, the model performs consistent across all thee
scenarios, indicating that it can effectively handle a broad range of V/t ratios also
outside the recommended industry guidelines range.



